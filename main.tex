%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[bioengineering,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aieduc, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefcommunication, briefreport, casereport, changes, clinicopathologicalchallenge, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, creative, datadescriptor, discussion, entry, expressionofconcern, extendedabstract, editorial, essay, erratum, fieldguide, hypothesis, interestingimages, letter, meetingreport, monograph, newbookreceived, obituary, opinion, proceedingpaper, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, supfile, systematicreview, technicalnote, viewpoint, guidelines, registeredreport, tutorial,  giantsinurology, urologyaroundtheworld
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

% Additional packages
\usepackage{algorithm}        % アルゴリズム環境
\usepackage{algorithmic}      % アルゴリズム記述
\usepackage{pdflscape}        % 横向きページ

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Feasibility Study of CLIP-Based Key Slice Selection in CT Images and Performance Enhancement via Lesion- and Organ-Aware Fine-Tuning}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Feasibility Study of CLIP-Based Key Slice Selection in CT Images and Performance Enhancement via Lesion- and Organ-Aware Fine-Tuning}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0009-0008-6043-4069} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0002-4222-4569} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Kohei Yamamoto $^{1}$*\orcidA{} and Tomohiro Kikuchi $^{2}$\orcidB{}}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Kohei Yamamoto, Tomohiro Kikuchi}

% MDPI internal command: Authors, for citation in the left column, only choose below one of them according to the journal style
% If this is a Chicago style journal 
% (arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci): 
% Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% If this is a APA style journal 
% (admsci, behavsci, businesses, econometrics, economies, education, ejihpe, games, humans, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth): 
% Lastname, F., Lastname, F., \& Lastname, F.

% If this is a ACS style journal (Except for the above Chicago and APA journals, all others are in the ACS format): 
% Lastname, F.; Lastname, F.; Lastname, F.
\isAPAStyle{%
       \AuthorCitation{Yamamoto, K. \& Kikuchi, T.}
         }{%
        \isChicagoStyle{%
        \AuthorCitation{Yamamoto, Kohei; Kikuchi, Tomohiro.}
        }{
        \AuthorCitation{Yamamoto, K.; Kikuchi, T.}
        }
}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Department of Radiology, School of Medicine, Jichi Medical University; yamamoto.kohei@jichi.ac.jp\\
$^{2}$ \quad Data Science Center, Jichi Medical University; r1419kt@jichi.ac.jp}

% Contact information of the corresponding author
\corres{Correspondence: yamamoto.kohei@jichi.ac.jp}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}  
% Current address should not be the same as any items in the Affiliation section.

%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes.

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{Large-scale medical visual question answering (MedVQA) datasets are critical for training and deploying vision-language models (VLMs) in radiology. Ideally, such datasets would be mined automatically from routine radiology reports and their accompanying images, yet no existing method links free-text findings to the relevant 2D slices in volumetric scans. We bridge this gap with a CLIP-based key-slice selector that matches each finding sentence to its most informative CT slice via text-image similarity. Our experiments show that models pre-trained on the medical domain already provide relatively good slice-retrieval accuracy, and that fine-tuning them on a small dual-supervised dataset that imparts both lesion- and organ-level awareness yields further gains. In particular, the best-performing model (BiomedCLIP fine-tuned) achieved a Top-1 accuracy of 51.7\% for lesion-aware slice retrieval, a 20-point improvement over baseline CLIP, and was accepted by radiologists in 56.3\% of cases. By automating report-to-slice alignment, the proposed method enables scalable, clinical-data-driven construction of MedVQA resources.
}

% Keywords
\keyword{CLIP; CT; key slice selection} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% 1. 現状のMedVQAの進展とその問題点
Recent advances in vision-language models (VLMs) have been driven by joint learning from paired image-text inputs \cite{li_llava-next-interleave_2024,xiao_florence-2_2024,chen_internvl_2024,alayrac_flamingo_2022}. 
In medical VLM research, medical visual question answering (MedVQA) is the de-facto benchmark task, where a model answers questions by jointly processing a medical image and the corresponding findings sentence(s) \cite{lau_dataset_2018,liu_slake_2021}. 
To scale training, most work mines figures and captions from large bibliographic databases and then uses large language models (LLMs) to synthesize VQA pairs \cite{zhang_pmc-vqa_2024,li_llava-med_2023}. 
While effective, these bibliographic-based datasets over-represent prototypical cases and may fail to capture the heterogeneity of real-world clinical presentations, potentially degrading model performance in practice \cite{zhang_pmc-vqa_2024,dong_generative_2025}. 
Consequently, constructing MedVQA datasets directly from routine clinical data would bridge this realism gap and could even exceed the scale of bibliographic-based collections. 
Although a VQA dataset has been curated from routine chest-radiograph reports, no comparable resource has yet been created for volumetric modalities such as CT or MRI \cite{bae_ehrxqa_2024}. 
One key reason is that reports are not annotated to show which sentence belongs to which slice, and we still lack automated tools to add those links, making it hard to build large, clinically sourced MedVQA datasets. 
Some studies have experimented with VLMs that use 3D encoders, pairing a full image volume with the entire report; however, these models require heavy computational resources, and many state-of-the-art (SOTA) VLMs still operate on 2D inputs \cite{bai_m3d_2024,blankemeier_merlin_2024,hamamci_ct2rep_2024}. 
Moreover, everyday radiology communication—whether in report snapshots, electronic medical record attachments, or conference slides—still revolves around just a few key 2D images, so slice-level tasks will remain indispensable in clinical practice. 
An automated key-slice selector that links each report sentence to the 2D slice that best matches its described finding is therefore essential for building the datasets needed to advance clinically useful VLMs in radiology.
The key motivation of this work is to establish such a method, bridging report sentences and CT slices to make the construction of clinically realistic MedVQA datasets feasible at scale.

% Fig. 1
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./figures/figure1_9.png}
  \caption{
    Overview of our CLIP-based slice-selection pipeline. 
    Each finding sentence is encoded by the text encoder and every CT slice by the image encoder. 
    Cosine similarities are computed between the sentence embedding and all slice embeddings, and the slice with the highest score is chosen as the key slice for that finding. 
}
  \label{fig:clip_overview}
\end{figure}


% 2. 本研究で提案するSlice Selectorの概要
In this study, we aim to clarify the utility of a slice-selection framework that aligns report sentences with CT slices and to examine whether fine-tuning with lesion- and organ-aware supervision can further enhance performance.
CLIP jointly embeds images and text into a common latent space and computes cosine similarity between these embeddings to perform zero-shot classiﬁcation and text-image retrieval \cite{radford_learning_2021}. 
We hypothesize that this same framework could be repurposed to align individual report sentences with their corresponding CT slices. 
Each finding sentence is embedded by the text encoder, and every axial slice in the volume is embedded by the image encoder; the slice with the highest cosine similarity is chosen as the key image for that sentence (Fig. \ref{fig:clip_overview}). 
We evaluate this pipeline with the original CLIP (ViT-B/16) and two medical-domain variants, PubMedCLIP and BiomedCLIP \cite{radford_learning_2021,eslami_pubmedclip_2023,zhang_biomedclip_2025}. 
In addition, we curated a compact training set drawn from a limited number of clinical CT studies and fine-tuned each model on this dataset. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Related Works}
\subsection{MedVQA Dataset Construction}
VQA-RAD is a foundational MedVQA benchmark for radiological images. 
Expert radiologists manually authored clinical questions and provided corresponding ground-truth answers, ensuring high annotation quality but resulting in a relatively small dataset \cite{lau_dataset_2018}. 
SLAKE builds on this by adding spatial annotations—such as pixel-level masks and bounding boxes—and by including knowledge-base-driven questions to probe deeper clinical reasoning \cite{liu_slake_2021}.
Effective training of VLMs in the highly specialized medical domain requires much larger and more diverse datasets. 
PMC-VQA constructs a 227K example VQA dataset, predominantly composed of radiological images, using literature extracted from PubMed Central \cite{zhang_pmc-vqa_2024}.
This dataset was built by mining images and their associated captions from the articles and employing ChatGPT to generate the corresponding VQA pairs.
LLava-Med adopts a similar strategy. It first trains on 600K image-caption pairs mined from PubMed Central articles. 
It then refines the model with 60K GPT-4-generated instruction-tuning examples \cite{li_llava-med_2023}.


% Extraction of slices from Volumetric Dataとか？
\subsection{Slice Selection}
Vote-MI proposes a method for extracting the most representative slices in 3D MRI volumes of brain tumor cases \cite{wang_enhancing_2024}. 
The method first employs a variational autoencoder to extract latent features from each slice, then applies clustering to select the key slice for each case. 
Training VLMs on the resulting dataset yields effective learning, although performance remains below that achieved with expert-curated datasets. 
Because Vote-MI does not incorporate any textual information during slice selection, its applicability is limited to brain tumor imaging and cannot readily extend to other anatomical regions or to multiple disease types.
Although no prior study has applied CLIP to slice-selection, related methods for extracting key frames from sequential medical data have been employed in zero-shot surgical phase recognition \cite{yuan_hecvl_2025}.


% CLIPの説明
\subsection{Multi Modal Learning}
With the advent of CLIP, contrastive learning has become a predominant paradigm for multimodal representation learning \cite{radford_learning_2021}. 
CLIP projects textual and visual inputs into a shared embedding space by encoding each modality separately—text via a transformer-based text encoder and images via a vision transformer—then aligns them through cosine similarity. 
This cross-modal alignment capability has prompted many researchers to employ the CLIP image encoder as the visual backbone in VLMs. 
Later works have built on this foundation to better support downstream tasks. For example, GLIP and RegionCLIP introduce region-level contrastive objectives that enhance object detection and semantic segmentation performance \cite{li_grounded_2022,zhong_regionclip_2022}.
SigLIP further innovates by replacing the standard contrastive loss with a sigmoid-cross-entropy formulation, substantially reducing text-encoder context length while maintaining high-efficiency multimodal learning \cite{zhai_sigmoid_2023}. 
CLIP-Adapter enhances few-shot transfer by inserting lightweight adapter modules into a frozen CLIP backbone, thereby boosting performance on downstream tasks without extensive retraining \cite{gao_clip-adapter_2024}. 
In the medical domain, PubMedCLIP fine-tunes CLIP on ROCO, a radiology-focused corpus derived from PubMed articles \cite{eslami_pubmedclip_2023,pelka_radiology_2018}. 
BiomedCLIP expands this approach by assembling a PMC-15M dataset from PubMed Central publications and using it for further CLIP fine-tuning \cite{zhang_biomedclip_2025}. 
These biomedical variants achieve SOTA performance on cross-modal retrieval, zero-shot image classification, and MedVQA tasks, establishing themselves as foundation models for a wide range of downstream medical-AI applications \cite{koleilat_medclip-sam_2024,zhao_foundation_2024,polis_exploring_2025}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Materials and Methods}

\subsection{Dataset}\label{sec:dataset}
% データセット概要
This study was approved by Jichi Medical University Hospital Bioethics Committee for Clinical Research (Approval No. 24-061). 
We assembled our dataset from 137 consecutive gastrointestinal cancer patients who underwent their ﬁrst CT examination at our institution between 2021 and 2023. 
First, a radiologist (Radiologist 1) with more than ten years of experience read each report and selected the appropriate CT series (e.g., the optimal contrast phase) for each finding sentence describing an abnormality. 
The radiologist then chose the single CT slice that best matched that sentence and drew a bounding box around the lesion. 
We divided the dataset into 92 training studies, 23 validation studies, and 22 test studies (Fig. \ref{fig:dataset_detail}). 
In the test set, the radiologist also annotated the range of slices corresponding to each sentence to allow more detailed evaluation. 
This yielded 625 sentence-slice pairs for training, 152 for validation, and 120 for testing. 
The original reports were written in Japanese; after confirming that they contained no protected health information, we translated all finding sentences into English using \texttt{GPT-4o-mini} \cite{openai_gpt-4_2024}. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./figures/dataset_detail_1.png}
  \caption{
    Overview of dataset details and evaluation measures.
  }
  \label{fig:dataset_detail}
\end{figure}

% totalsegmentatorによるデータ拡張手法
To improve performance on negative findings, we augmented the dataset with synthetic finding-image pairs that contain no abnormal findings. 
We first applied TotalSegmentator to each CT series to extract organ masks and determine which of the 15 major thoracoabdominal organs were present for each slice \cite{wasserthal_totalsegmentator_2023}. 
Using these organ labels, we generated pseudo-findings in two ways: by inserting organ names into a fixed template and by prompting \texttt{GPT-4o-mini} to produce natural-language descriptions based on the same templates. 
Further details of our rule-based and LLM-based prompt designs are provided in Appendix \ref{app:rule_based_text_prompt}.


% 最終的な学習データセット
For lesion-positive examples, we expanded the slice range based on lesion size: bounding boxes larger than 2,000 pixels were padded by one slice above and below, while boxes exceeding 4,000 and 6,000 pixels received two- and three-slice expansions, respectively. 
This procedure yielded a training set composed of 12,743 CT slices, including 1,009 lesion-positive finding-image pairs and 140,761 normal-anatomy pairs generated from organ labels.
As part of image preprocessing, we applied soft-tissue windowing to the CT values to enhance contrast in relevant intensity ranges. 
Because ViT backbone requires 224 × 224 inputs, we first cropped 32 pixels from each edge of the original 512 × 512 slices to obtain 448 × 448 images and then downsampled them to 224 × 224. 
During training, we applied data augmentations such as horizontal and vertical flips, translations, scaling, rotations, elastic distortions, and cutout to improve model robustness \cite{devries_improved_2017}.


\subsection{Slice-Selection Algorithm}  
To operationalize our approach, we define a slice-selection algorithm that links each finding sentence to its most relevant CT slice. 
As illustrated in Fig. \ref{fig:clip_overview}, the text encoder embeds the finding sentence, while the image encoder embeds each axial slice of the CT volume. 
Cosine similarities are then computed between the sentence embedding and all slice embeddings. 
The slice with the highest similarity score is designated as the key slice corresponding to the given finding. 
This procedure, summarized in Alg. \ref{alg:key_slice_selection}, constitutes the inference pipeline of our method and serves as the foundation for both evaluation and fine-tuning.

\begin{algorithm}
  \caption{CLIP-based Key-Slice Selection}
  \label{alg:key_slice_selection}
  \begin{algorithmic}[1]
    \REQUIRE finding sentence $f$, CT volume $V = \{s_1, \dots, s_N\}$, text encoder $E_{\mathrm{text}}$, image encoder $E_{\mathrm{img}}$
    \ENSURE key slice $s_k$
    \STATE $e_{\mathrm{text}} \leftarrow E_{\mathrm{text}}(f)$ 
    \FOR{$i \leftarrow 1$ to $N$}
      \STATE $e_{\mathrm{img}} \leftarrow E_{\mathrm{img}}(s_i)$ 
      \STATE $\mathrm{sim}_i \leftarrow \cos\bigl(e_{\mathrm{text}}, e_{\mathrm{img}}\bigr)$ 
    \ENDFOR
    \STATE $k \leftarrow \arg\max_i \mathrm{sim}_i$ 
    \RETURN $s_k$
  \end{algorithmic}
\end{algorithm}


% 学習手法
\subsection{Models and Training Details}
We trained CLIP to learn correspondences between finding sentences and CT slices. 
For the image encoder, we employed \texttt{ViT-B/16} and fine-tuned the pre-trained model using weights provided by OpenAI. 
We applied the same procedure to PubMedCLIP and BiomedCLIP.
For all text encoders, the context length was set to 77 tokens, and sentences exceeding this length were truncated.
All training took place on a single NVIDIA RTX 6000 Ada GPU with a batch size of 64, using the AdamW optimizer \cite{loshchilov_fixing_2017}. 
We decayed the learning rate from 5e-5 to 1e-6 over 20 epochs using cosine annealing. 
Finally, we selected the checkpoint with the lowest validation loss for our evaluations.
The experiments were run on Ubuntu 22.04.5 LTS with Python~3.11.9 (conda-forge), using \texttt{numpy}~2.1.1, \texttt{torch}~2.4.1+cu118, and \texttt{TotalSegmentator}~2.10.0.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

% セクション全体の説明
We evaluated the slice selection performance of six models: the pre-trained CLIP, PubMedCLIP, and BiomedCLIP, along with their fine-tuned counterparts (FT). 
In Section \ref{sec:lesion_aware}, we assess retrieval accuracy on sentences describing actual abnormal findings from diagnostic reports. 
In addition, we conducted a radiologist evaluation of the slice-selection results to examine clinical applicability.
Section \ref{sec:organ_aware} reports the alignment fidelity between organ-related text and its corresponding CT slices. 
% In both settings, we compare two similarity scoring schemes: direct use of cosine similarity between text and image embeddings, 
% and a case-specific scaling of similarity scores to account for inter-case variability. 
Finally, Section \ref{sec:visualization} presents visualizations of the slice selection results on key series, illustrating both quantitative performance and qualitative interpretability.


\subsection{Lesion Awareness}\label{sec:lesion_aware}
We evaluated the models on a test set of 22 studies containing 120 finding sentences with manually annotated ground-truth slice ranges. 
At inference time, each sentence was encoded by the CLIP text encoder, every slice in the corresponding CT volume was processed by the image encoder, cosine similarities were computed, and the slice with the highest score was taken as the model-selected key slice (Fig. \ref{fig:clip_overview}, Alg. \ref{alg:key_slice_selection}). 
To assess inter-observer variability, Radiologist 2—independent of the ground-truth annotator—also selected the single best-matching slice for each sentence.
Table \ref{tab:slice_extraction_accuracy} shows both the proportion of Top-1 predictions falling within the annotated ranges and the proportion of cases where any of the Top-5 most similar slices overlap with those ranges. 
CLIP (FT) improved Top-1 accuracy by 20 percentage points over the CLIP, demonstrating effective adaptation to the medical domain.
Both PubMedCLIP and BiomedCLIP began with higher baseline accuracies and further improved after fine-tuning. 
In particular, BiomedCLIP (FT) achieved the best results, with a Top-1 accuracy of 51.72\% and a Top-5 accuracy of 64.37\%. 
Considering that a second radiologist achieved a Top-1 accuracy of 78.16\%, the model’s 51.72\% can be viewed as a relatively strong result.



\begin{table}[ht]
  \centering
  \caption{Comparison of slice-selection accuracy for each method. 
  Reported values are Top-1 and Top-5 accuracy[\%]. 
  For each metric, the highest score among the automated methods (excluding the Radiologist2) is highlighted in red.}
  \label{tab:slice_extraction_accuracy}
  \begin{tabular}{lcc}
    \toprule
    Method                  & Acc.@1\,↑      & Acc.@5\,↑      \\
    \midrule
    Radiologist2            & 78.16       & -           \\
    CLIP                    & 19.54       & 44.83       \\
    CLIP(FT)                & 40.23       & 49.43       \\
    PubMedCLIP              & 29.89       & 54.02       \\
    PubMedCLIP(FT)          & 42.53       & 59.77       \\
    BiomedCLIP              & 44.83       & 60.92       \\
    BiomedCLIP(FT)          & \textcolor{red}{51.72} & \textcolor{red}{64.37} \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[ht]
  \centering
  \caption{Radiologist acceptance rates[\%] of slice selector predictions for each model.}
  \label{tab:slice_selector_adoption}
  \begin{tabular}{lc}
    \toprule
    Method                  & Acceptance Rate\,↑ \\
    \midrule
    CLIP                    & 24.14        \\
    CLIP(FT)                & 40.23        \\
    PubMedCLIP              & 37.93        \\
    PubMedCLIP(FT)          & 50.57        \\
    BiomedCLIP              & 50.57        \\
    BiomedCLIP(FT)          & 56.32        \\
    \bottomrule
  \end{tabular}
\end{table}


% \section{Radiologist Evaluation of Slice Selection Results}\label{app:doctor_evaluation}
As noted above, we recognized that strict ground-truth ranges do not always align with radiologist subjective assessments.
Therefore, to evaluate whether the slices proposed by each method would be considered acceptable in clinical practice, we conducted a qualitative assessment in which a radiologist evaluated whether to adopt each suggested slice outright for a given finding sentence. 
We developed a simple web application that displayed each finding sentence alongside the slices proposed by each slice selector. 
For each case, suggestions from all models were presented in random order on the same screen, and the radiologist was blinded to the source model. 
Table \ref{tab:slice_selector_adoption} reports the adoption rate for each model’s recommendations. 
While these adoption rates generally correspond to the quantitative results shown in Table \ref{tab:slice_extraction_accuracy} (Top-1 accuracy), they are uniformly slightly higher. 
These higher adoption rates arise when lesions that the radiologist still finds acceptable fall outside the strict ground-truth range—for example, calcifications that extend across several slices or cases where the annotation covers only one of multiple hepatic cysts.


\subsection{Organ Awareness}\label{sec:organ_aware}
Recognizing normal anatomy is as important as detecting abnormalities, so we deﬁne organ-aware slice selection as a complementary evaluation. 
Using TotalSegmentator, we generated binary organ presence labels for each CT slice based on 15 major thoracoabdominal organs \cite{wasserthal_totalsegmentator_2023}. 
For each model, we encoded the organ prompt and every CT slice, computed their cosine similarities, and then determined organ-specific thresholds (via the Youden index on the validation set) that served as the cut-off for deciding whether a given organ was present in each slice on test set \cite{youden_index_1950}. 
To explore the impact of text granularity, we compared two input types: single-word organ names (e.g., “Heart,” “Liver”) and full template sentences such as “This CT image includes the heart.” 
These experiments inform whether to use word-level or sentence-level prompts when querying the text encoder. 


Table \ref{tab:organ_extraction_results} summarizes the organ-aware evaluation, reporting both accuracy and F1-score for each model under Word and Sentence prompt conditions. 
BiomedCLIP (FT) achieved the highest performance in both settings, with an F1-score of 0.85 for Word and Sentence inputs. 
PubMedCLIP exhibited similar accuracy and F1-scores across the two prompt types, whereas BiomedCLIP tended to perform better with Sentence prompts. 
Fine-tuning yielded roughly equivalent gains for Word and Sentence inputs across all models, indicating that our approach generalizes well to both single-token organ names and full descriptive sentences. 
On average, fine-tuning improved accuracy and F1-score by more than 10\% for every model. 
These substantial improvements not only demonstrate enhanced normal-anatomy recognition accuracy for the original CLIP model but also for the medical-domain pre-trained variants PubMedCLIP and BiomedCLIP. 
They suggest that fine-tuning can improve slice extraction accuracy from non-abnormal findings and, consequently, benefit downstream VQA generation.
F1-scores for each organ are provided in Appendix \ref{app:organ_aware_details}.


\begin{table}[ht]
  \centering
  \caption{Comparison of organ extraction performance. Reported metrics are accuracy (Acc.) and F1-score (F1). 
  “Word” indicates prompts using single organ names, and “Sentence” indicates prompts using full finding sentences. 
  The highest score in each column is highlighted in red.}
  \label{tab:organ_extraction_results}
  \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Word}               & \multicolumn{2}{c}{Sentence}           \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
                           & Acc.\,↑ & F1\,↑         & Acc.\,↑   & F1\,↑         \\ 
    \midrule
    CLIP                    & $0.715\pm0.105$ & $0.514\pm0.182$ & $0.725\pm0.071$ & $0.520\pm0.192$ \\
    CLIP(FT)                & $0.927\pm0.038$ & $0.806\pm0.153$ & $0.934\pm0.035$ & $0.824\pm0.129$ \\
    PubMedCLIP              & $0.881\pm0.046$ & $0.722\pm0.164$ & $0.882\pm0.042$ & $0.726\pm0.157$ \\
    PubMedCLIP(FT)          & $0.943\pm0.030$ & $0.839\pm0.123$ & $0.944\pm0.031$ & $0.843\pm0.121$ \\
    BiomedCLIP              & $0.832\pm0.133$ & $0.673\pm0.230$ & $0.884\pm0.048$ & $0.730\pm0.126$ \\
    BiomedCLIP(FT)          & \textcolor{red}{$0.947\pm0.033$} & \textcolor{red}{$0.853\pm0.118$} & \textcolor{red}{$0.948\pm0.032$} & \textcolor{red}{$0.854\pm0.118$} \\
    \bottomrule
  \end{tabular}
\end{table}


Additionally, as an external dataset benchmark, we evaluated organ awareness using the publicly available CT-RATE dataset, which consists of chest CT volumes with organ labels generated by TotalSegmentator \cite{hamamci_ct2rep_2024}. 
We conducted evaluation on 1,304 cases from the validation split, using the earliest series (denoted as “\_a\_1” in the dataset) when multiple CT series were available per patient. 
The results are summarized in Table \ref{tab:organ_extraction_results_ctrate}. 
Overall performance was lower than those reported in Table \ref{tab:organ_extraction_results}, likely due to domain shifts related to imaging devices and acquisition parameters. 
For instance, BiomedCLIP(FT) achieved an accuracy of 0.926 and an F1-score of 0.809 on CT-RATE, compared to 0.948 and 0.854, respectively, in our in-house dataset. 
Despite the overall decrease, BiomedCLIP(FT) consistently yielded the highest accuracy among all models, confirming its robustness across datasets.


\begin{table}[ht]
  \centering
  \caption{Comparison of organ extraction performance on the external CT-RATE dataset. 
  Reported metrics are accuracy (Acc.) and F1-score (F1). 
  “Word” indicates prompts using single organ names, and “Sentence” indicates prompts using full finding sentences. 
  The highest score in each column is highlighted in red.}
  \label{tab:organ_extraction_results_ctrate}
  \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Word}               & \multicolumn{2}{c}{Sentence}           \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
                           & Acc.\,↑ & F1\,↑         & Acc.\,↑   & F1\,↑         \\ 
    \midrule
    CLIP                    & $0.726\pm0.059$ & $0.554\pm0.223$ & $0.724\pm0.065$ & $0.556\pm0.226$ \\
    CLIP(FT)                & $0.902\pm0.050$ & $0.770\pm0.251$ & $0.902\pm0.048$ & $0.770\pm0.250$ \\
    PubMedCLIP              & $0.842\pm0.099$ & $0.680\pm0.237$ & $0.861\pm0.095$ & $0.707\pm0.226$ \\
    PubMedCLIP(FT)          & $0.911\pm0.072$ & $0.796\pm0.252$ & $0.910\pm0.073$ & $0.795\pm0.252$ \\
    BiomedCLIP              & $0.749\pm0.171$ & $0.632\pm0.271$ & $0.755\pm0.125$ & $0.604\pm0.226$ \\
    BiomedCLIP(FT)          & \textcolor{red}{$0.926\pm0.059$} & \textcolor{red}{$0.809\pm0.257$} & \textcolor{red}{$0.926\pm0.059$} & \textcolor{red}{$0.809\pm0.257$} \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Visualization}\label{sec:visualization}
Fig. \ref{fig:lesion_aware_results} shows an example of lesion-aware slice-selection result on our test cohort. 
For a case of uterine fibroids, PubMedCLIP, BiomedCLIP, and BiomedCLIP (FT) all correctly identify the key CT slice containing the lesion.
Fig. \ref{fig:organ_aware_results} shows similarity profiles between organ-related text and CT slices for a selected series. 
The original CLIP model exhibits numerous false positives and inconsistent alignment, whereas CLIP (FT) produces a much cleaner correspondence and improved localization. 
Although PubMedCLIP and BiomedCLIP also display some false positives and false negatives, both models achieve high accuracy in recognizing the target organ after fine-tuning. 
Additional qualitative examples are provided in Appendix \ref{app:visualization}.


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/figure2_4.png}
  \caption{"Multiple masses that may represent fibroids are observed in the myometrium and beneath the endometrium."
  The Ground Truth column shows the reference CT slice with the radiologist-annotated lesion bounding box.
  The other columns display each model’s key-slice prediction.
  The white double circle symbol indicates predictions that match the ground-truth slice range.
  }
  \label{fig:lesion_aware_results}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/figure3_4.png}
  \caption{Similarity profile between the prompt “Heart” and each slice. 
  The green line denotes the ground-truth slice range. 
  Similarity profiles increase from left to right, and the dashed line indicates the threshold. 
  }
  \label{fig:organ_aware_results}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

In this study, we introduced a CLIP-based key-slice selector that aligns free-text radiology findings with their corresponding CT slices. 
We evaluated our approach along two complementary axes: lesion awareness and organ awareness. 
Despite training on a relatively small dataset, fine-tuned models achieved substantial gains in lesion-aware slice retrieval—improving accuracy by 7 to 20 percentage points over the pre-trained baselines. 
The best performer, BiomedCLIP (FT), reached 51\% Top-1 accuracy for lesion localization. 
In the organ-aware task, all fine-tuned models realized over 10\% improvements in both accuracy and F1-score. 
Notably, performance remained strong whether organ prompts were provided as single words or full sentences, demonstrating the flexibility of our text-encoding strategy. 
Moreover, to examine robustness under external conditions, we conducted an additional organ-awareness evaluation on the publicly available CT-RATE dataset. 
While all methods exhibited some degradation in performance due to domain shifts in imaging devices and acquisition protocols, the decrease was relatively modest, and BiomedCLIP(FT) consistently achieved the highest accuracy. 
These findings suggest that our approach maintains robustness across datasets and can generalize beyond our in-house cohort.

In terms of system requirements, all experiments were conducted on a single NVIDIA RTX 6000 Ada GPU with 48 GB memory, which was sufficient to train the models with a batch size of 64. 
Each training run of 20 epochs finished within one hour. 
These results indicate that our method can be reproduced on a high-end single-GPU workstation without requiring distributed resources.

Beyond our own dataset, the proposed slice-selection framework offers practical benefits when applied to diverse imaging archives. 
First, by automatically aligning report sentences with key slices, it can substantially reduce the cost of 2D slice-level annotation, which is often the major bottleneck in dataset curation. 
Second, it enables the construction of image-text pairs directly from unlabeled CT volumes and routine reports, providing a low-cost pathway to expand training resources for medical VLMs. 
These improvements highlight the broader utility of our method across datasets and institutions, extending its impact beyond the present feasibility study.

Our method paves the way for fully automated construction of MedVQA datasets from routine clinical archives, directly addressing biases and scale limitations inherent in bibliographic-based collections. 
Furthermore, the radiologist evaluation of slice-selection results (Table \ref{tab:slice_selector_adoption}) and the visualization of similarity profiles (Fig. \ref{fig:organ_aware_results}) suggest that, in addition to serving as a VQA dataset generator, integrating the slice selector into clinical workflows as a recommendation tool could help reduce radiologists’ workload and enhance diagnostic accuracy. 


\textbf{Limitations}
First, because this is a feasibility study, the training and test cohorts are small and drawn from a single institution. Expanding the dataset size could uncover additional gains in retrieval accuracy and reduce the risk of overﬁtting to our relatively small evaluation cohorts. 
Second, our evaluation reports only overall lesion- and organ-aware metrics; it does not stratify results by lesion size, anatomical region, contrast phase, or disease category. A finer-grained analysis is essential for uncovering failure modes in clinically relevant subgroups. 
Third, the Youden-index thresholds used to decide organ presence were chosen on the validation set and then fixed for testing. Although this mirrors a real-world deployment scenario, alternative calibration strategies might yield better threshold stability on unseen data. 


\textbf{Future Work}
In the future, we will integrate the key-slice selector into a complete MedVQA-generation pipeline. 
This pipeline consists of four main stages: 
\begin{enumerate}
  \item segmenting diagnostic reports into individual finding units, 
  \item acquiring the corresponding imaging series (including selection of contrast phases),
  \item extracting key slices using our slice selector, and 
  \item generating VQA pairs from each finding sentence through rule-based or LLMs.
\end{enumerate}
While our study has addressed stage 3, 
the remaining steps are equally critical for end-to-end automation.
Accordingly, we plan to develop and evaluate methods for report segmentation and image retrieval, 
and to implement VQA generation using both deterministic templates and generative LLM prompts. 
As these components mature, we will iteratively expand our clinical dataset and refine the slice selector’s accuracy. 
% Finally, we intend to train VLMs on the resulting automatically constructed MedVQA dataset and compare their performance against 
% models trained on manually curated slice annotations to quantify the impact of automated slice selection on downstream VLMs capabilities.
We also plan to train VLMs on the resulting automatically constructed MedVQA dataset and compare their performance against 
models trained on manually curated slice annotations to quantify the impact of automated slice selection on downstream VLM capabilities.

Beyond dataset construction, architectural advances offer another promising direction. 
Recent large-scale VLMs such as Merlin, which align 3D CT volumes with radiology reports through contrastive learning, 
provide useful design principles\cite{blankemeier_merlin_2024}. 
Incorporating similar volume-level representation strategies, as well as techniques from the CLIP-Driven Universal Model that use text embeddings as semantic labels for segmentation and detection, 
could extend our framework beyond slice retrieval toward finer-grained localization and more efficient MedVQA dataset construction\cite{10376801}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

This study presented a CLIP-based key-slice selector that aligns free-text radiology findings with their corresponding CT images. Fine-tuning on a compact dual-supervised dataset led to a notable improvement in lesion-aware accuracy, with the best-performing model achieving strong results in lesion localization. In the organ-aware task, all fine-tuned models demonstrated improvements in both accuracy and F1-score, regardless of whether prompts were single words or full sentences. A qualitative review indicated that the slice selector’s recommendations could reduce radiologists’ workload and enhance diagnostic confidence in routine practice. However, the current evaluation is limited by a single-institution, small-cohort design, the absence of lesion-size or disease-specific analyses, and the use of fixed threshold calibration, all of which limit generalisability. Future work will expand the dataset, incorporate stratified analyses, and integrate the selector into an end-to-end MedVQA pipeline that includes report segmentation, series retrieval, slice extraction, and automatic question-answer generation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following supporting information can be downloaded at:  \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.}

% Only for journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title. A supporting video article is available at doi: link.}

% Only used for preprtints:
% \supplementary{The following supporting information can be downloaded at the website of this paper posted on \href{https://www.preprints.org/}{Preprints.org}.}

% Only for journal Hardware:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.\vspace{6pt}\\
%\begin{tabularx}{\textwidth}{lll}
%\toprule
%\textbf{Name} & \textbf{Type} & \textbf{Description} \\
%\midrule
%S1 & Python script (.py) & Script of python source code used in XX \\
%S2 & Text (.txt) & Script of modelling code used to make Figure X \\
%S3 & Text (.txt) & Raw data from experiment X \\
%S4 & Video (.mp4) & Video demonstrating the hardware in use \\
%... & ... & ... \\
%\bottomrule
%\end{tabularx}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, K.Y. and T.K.; methodology, K.Y.; software, K.Y.; validation, K.Y.; formal analysis, K.Y.; investigation, K.Y. and T.K.; resources, T.K.; data curation, K.Y. and T.K.; writing---original draft preparation, K.Y.; writing---review and editing, T.K.; visualization, K.Y.; supervision, T.K.; project administration, K.Y. All authors have read and agreed to the published version of the manuscript.}

\funding{This research received no external funding.}

\institutionalreview{The study was conducted in accordance with the Declaration of Helsinki, and approved by the Jichi Medical University Hospital Bioethics Committee for Clinical Research (protocol code 24-061 and date of approval 25 September 2024).}

\informedconsent{This retrospective study was approved by the Jichi Medical University Hospital Bioethics Committee for Clinical Research, and the requirement for informed consent was waived due to the retrospective nature of the study and the use of anonymized medical imaging data that poses minimal risk to participants.}

\dataavailability{The datasets generated and analyzed during the current study are not publicly available due to privacy and ethical restrictions related to patient medical imaging data. The source code used for the analysis is available from the corresponding author on reasonable request.}

\conflictsofinterest{The authors declare no conflicts of interest.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional

%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

\abbreviations{Abbreviations}{
The following abbreviations are used in this manuscript:
\\

\noindent 
\begin{tabular}{@{}ll}
CLIP & Contrastive Language-Image Pre-training\\
CT & Computed Tomography\\
VLM & Vision-Language Model\\
MedVQA & Medical Visual Question Answering\\
LLM & Large Language Model\\
SOTA & State-of-the-Art\\
VQA & Visual Question Answering\\
\end{tabular}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixstart
\appendix
\section{Findings Text Generation}\label{app:rule_based_text_prompt}
The following templates were designed to generate non-abnormal finding sentences using organ labels predicted by TotalSegmentator. 
In each template, the placeholder \texttt{\{\{organ\}\}} is replaced with the organ label obtained from TotalSegmentator. 
Note that the original templates are written in Japanese and then translated into English, so their word order may differ from natural English syntax.
\subsection{Rule-based}
\begin{verbatim}
  organs = [
      "aorta",
      "colon",
      "duodenum",
      "esophagus",
      "gallbladder",
      "heart",
      "kidney",
      "liver",
      "pancreas",
      "stomach",
      "spleen",
      "thyroid_gland",
      "urinary_bladder",
      "trachea",
      "lung",
  ]
  text_templates = {
    "prefix": [
      "In the image,",
      "In this image,",
      "In this CT image,",
      "",
    ],
    "suffix": [
      "{{organ}} is visible."
      "{{organ}} is included."
      "{{organ}} is contained."
      "{{organ}} etc. are visible."
      "{{organ}} etc. are included."
      "{{organ}} etc. are contained."
    ],
  }
\end{verbatim}
\subsection{LLMs-based}
Since we generated 20 templates for each organ, we present only those for the liver and pancreas here.
\begin{verbatim}
  {
    'liver': [
      'No abnormal findings are observed in the liver, and normal morphology is maintained.',
      'No abnormal findings are observed in the liver, \
      and morphology falls within normal limits.',
      'No lesions or abnormal findings are noted in the liver.',
      'No abnormal findings are observed in the liver, and normal morphology is maintained.',
      'No abnormal findings are observed in the liver, and it remains within normal limits.',
      'No abnormal masses or tumors can be identified in the liver.',
      'No abnormal findings are observed in the liver, and its structure is normal.',
      'No abnormal findings are observed in the liver, and its normal state is preserved.',
      'No abnormal findings are observed in the liver, \
      and its shape and size are within normal limits.',
      'No abnormal masses or fatty changes are noted in the liver.',
      'No abnormal masses or deformities are observed in the liver, \
      and findings are within normal limits.',
      'No findings indicative of abnormality are observed in the liver.',
      'No deformities or neoplastic lesions are observed in the liver, \
      which maintains normal morphology.',
      'No morphological abnormalities or neoplastic lesions are observed in the liver.',
      'No significant lesions or impairments are detected in the liver.',
      'No particular abnormalities are observed in the liver, \
      which preserves normal morphology.',
      'No abnormalities in shape or echo texture are noted in the liver.',
      'No obvious lesions or abnormal findings are noted in the liver.',
      'No abnormal masses or perfusion changes are observed in the liver.',
      'No abnormal findings are observed in the liver, and its normal structure is preserved.'
    ],
  'pancreas': [
    'No abnormal findings are observed in the pancreas.',
    'No findings suggestive of abnormality are noted in the pancreas.',
    'No significant abnormalities are observed in the pancreas.',
    'No findings indicating abnormality are apparent in the pancreas.',
    'No abnormal findings are observed in the pancreas, and normal morphology is maintained.',
    'No abnormal masses or signs of inflammation are observed in the pancreas.',
    'No masses or inflammatory findings are noted in the pancreas.',
    'No abnormal findings are observed in the pancreas, \
    and its morphology falls within normal limits.',
    'No tumors or inflammatory findings are observed in the pancreas, \
    which retains normal morphology.',
    'No findings suggestive of abnormality are apparent in the pancreas.',
    'No structural abnormalities or lesions are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas.',
    'No tumors or abnormal structural changes are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas, \
    and normal imaging appearance is obtained.',
    'No abnormal findings are observed in the pancreas, \
    and normal morphology is confirmed.',
    'No abnormal masses or inflammation are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas.',
    'No abnormal findings of any kind are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas, and normal morphology is present.',
    'No findings suggestive of lesions are observed in the pancreas.'
    ]
  }
\end{verbatim}


% \section{Radiologist Evaluation of Slice Selection Results}\label{app:doctor_evaluation}
% In addition to supporting automated MedVQA dataset creation, our slice selector can serve as a clinical decision support tool by 
% recommending the most relevant CT slices to radiologists. 
% To assess this utility, we conducted a qualitative evaluation in which a radiologist reviewed the slice suggestions generated by 
% each model for a series of finding sentences. 
% Rather than measuring strict overlap with annotated ground truth, 
% the radiologist indicated whether they would accept the recommended slice as appropriate for the given finding.
% The radiologist who participated in this evaluation was not the same individual who performed the dataset annotations. これは菊地先生だわ。
% We developed a simple web application to display each finding sentence alongside the proposed slices. 
% For each case, the suggestions from all models were presented in random order, and the radiologist was blinded to model identity. 
% Table \ref{tab:slice_selector_adoption} reports the adoption rate for each model’s recommendations, 
% defined as the percentage of suggestions the radiologist deemed acceptable.
% The adoption rates generally mirror the quantitative Top-1 accuracy results shown in Table \ref{tab:slice_extraction_accuracy}, 
% although overall rates tended to be slightly higher. 
% We attribute this to lesions such as calcifications that span broader regions: even when a suggested slice falls outside the strict ground-truth interval, 
% it may still be clinically acceptable to the radiologist.


\clearpage
\begin{landscape}


\section{Detailed Organ-Aware Results}\label{app:organ_aware_details}


\begin{table}[ht]
  \centering
  \caption{F1 scores for each organ across slice selection methods. \\
  “Word” denotes prompts using only the organ name, 
  and “Sentence” denotes prompts using full finding sentences.}
  \label{tab:organ_f1_comparison}
  \footnotesize
  \begin{tabular}{l*{6}{cc}}
    \toprule
    Organ
      & \multicolumn{2}{c}{CLIP}
      & \multicolumn{2}{c}{CLIP(FT)}
      & \multicolumn{2}{c}{PubMedCLIP}
      & \multicolumn{2}{c}{PubMedCLIP(FT)}
      & \multicolumn{2}{c}{BiomedCLIP}
      & \multicolumn{2}{c}{BiomedCLIP(FT)} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}%
    \cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}
      & word & sentence
      & word & sentence
      & word & sentence
      & word & sentence
      & word & sentence
      & word & sentence \\
    \midrule
    aorta             & 0.679 & 0.660 & 0.960 & 0.960 & 0.852 & 0.836 & 0.966 & 0.967 & 0.858 & 0.754 & 0.943 & 0.948 \\
    colon             & 0.676 & 0.747 & 0.877 & 0.877 & 0.817 & 0.859 & 0.880 & 0.878 & 0.878 & 0.886 & 0.878 & 0.878 \\
    duodenum          & 0.450 & 0.522 & 0.761 & 0.775 & 0.749 & 0.762 & 0.804 & 0.841 & 0.665 & 0.724 & 0.798 & 0.828 \\
    esophagus         & 0.614 & 0.731 & 0.932 & 0.931 & 0.936 & 0.918 & 0.958 & 0.958 & 0.873 & 0.895 & 0.973 & 0.972 \\
    gallbladder       & 0.250 & 0.235 & 0.494 & 0.495 & 0.450 & 0.472 & 0.547 & 0.547 & 0.432 & 0.451 & 0.543 & 0.537 \\
    heart             & 0.554 & 0.643 & 0.933 & 0.941 & 0.842 & 0.832 & 0.945 & 0.943 & 0.664 & 0.841 & 0.956 & 0.955 \\
    kidney            & 0.684 & 0.576 & 0.886 & 0.887 & 0.767 & 0.788 & 0.885 & 0.884 & 0.733 & 0.696 & 0.917 & 0.917 \\
    liver             & 0.600 & 0.570 & 0.900 & 0.901 & 0.791 & 0.832 & 0.928 & 0.921 & 0.821 & 0.750 & 0.914 & 0.912 \\
    pancreas          & 0.588 & 0.518 & 0.801 & 0.807 & 0.731 & 0.716 & 0.836 & 0.841 & 0.730 & 0.755 & 0.838 & 0.824 \\
    stomach           & 0.402 & 0.418 & 0.720 & 0.737 & 0.650 & 0.612 & 0.780 & 0.767 & 0.699 & 0.673 & 0.823 & 0.827 \\
    spleen            & 0.325 & 0.409 & 0.731 & 0.744 & 0.619 & 0.685 & 0.809 & 0.811 & 0.665 & 0.639 & 0.810 & 0.800 \\
    thyroid gland     & 0.363 & 0.227 & 0.744 & 0.744 & 0.552 & 0.576 & 0.648 & 0.647 & 0.219 & 0.643 & 0.727 & 0.728 \\
    urinary bladder   & 0.165 & 0.210 & 0.491 & 0.691 & 0.387 & 0.386 & 0.739 & 0.779 & 0.160 & 0.575 & 0.753 & 0.762 \\
    trachea           & 0.542 & 0.514 & 0.900 & 0.905 & 0.730 & 0.714 & 0.881 & 0.884 & 0.796 & 0.767 & 0.945 & 0.947 \\
    lung              & 0.816 & 0.827 & 0.964 & 0.963 & 0.954 & 0.904 & 0.976 & 0.975 & 0.906 & 0.907 & 0.980 & 0.981 \\
    \bottomrule
  \end{tabular}
\end{table}
\end{landscape}


\section{Visualization}\label{app:visualization}
\subsection{Lesion Awareness}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst6_2.png}
  \caption{Visualization of lesion-aware slice selection for the finding 
  "Circumferential wall thickening is observed in the antrum to pylorus of the stomach, which is consistent with findings of gastric cancer. 
  There is evidence of extramural invasion."
  The Ground Truth column shows the reference CT slice with the radiologist-annotated lesion bounding box.
  The other columns display each model’s key-slice prediction.
  The white double circle symbol indicates predictions that match the ground-truth slice range.}
  \label{fig:lesion_aware_inst6}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst7_2.png}
  \caption{Visualization of lesion-aware slice selection for the finding 
  "Multiple enlarged lymph nodes are observed in the celiac artery region, which is considered metastatic."
  For this example, no prediction fell within the ground-truth range.}
  \label{fig:lesion_aware_inst7}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst8_2.png}
  \caption{Visualization of lesion-aware slice selection for the finding 
  "Ring enhancement lesions are observed in both lobes of the liver, indicating the presence of hepatic metastases."}
  \label{fig:lesion_aware_inst8}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst10_2.png}
  \caption{Visualization of lesion-aware slice selection for the finding "Prostate enlargement is observed."}
  \label{fig:lesion_aware_inst10}
\end{figure}


\clearpage


\subsection{Organ Awareness}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_organ_aware_liver_2.png}
  \caption{Similarity profile between the prompt “Liver” and each axial CT slice. 
  The green line denotes the ground-truth slice range. 
  The dashed line indicates the Youden-index threshold determined on the validation set.}
  \label{fig:organ_aware_liver}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_organ_aware_kidney_2.png}
  \caption{Similarity profile between the prompt “Kidney” and each axial CT slice.}
  \label{fig:organ_aware_kidney}
\end{figure}

\clearpage

\section{Impact of Organ-Aware Synthetic Data}\label{app:impact}
To highlight the contribution of synthetic data generated from organ labels, we conducted an ablation study in which the models were fine-tuned only on report-derived lesion sentences, without using any organ-aware synthetic data. 
Because the training set in this condition contained only 625 sentence-slice pairs (Fig. \ref{fig:dataset_detail}), we adopted a milder fine-tuning schedule with a learning rate decayed from 1e-5 to 1e-6 over 20 epochs. 
We refer to this model as BiomedCLIP(lesion-FT). 
Table \ref{tab:impact} compares the lesion-aware accuracy of BiomedCLIP, BiomedCLIP(lesion-FT), and BiomedCLIP(FT).
The results show that BiomedCLIP(lesion-FT) achieved a higher Top-1 accuracy than the original BiomedCLIP baseline (45.98\% vs. 44.83\%) but remained inferior to BiomedCLIP(FT), which incorporated organ-aware synthetic data (51.72\%).
These findings indicate that synthetic data are beneficial not only for organ-aware evaluation but also for improving lesion-aware performance.

\begin{table}[ht]
  \centering
  \caption{Lesion-aware accuracy [\%] of BiomedCLIP variants. BiomedCLIP(lesion-FT) was fine-tuned only on lesion sentences without synthetic data.}
  \label{tab:impact}
  \begin{tabular}{lcc}
    \toprule
    Method                  & Acc.@1\,↑      & Acc.@5\,↑      \\
    \midrule
    BiomedCLIP              & 44.83       & 60.92       \\
    BiomedCLIP(FT)          & 51.72       & 64.37       \\
    BiomedCLIP(lesion-FT)   & 45.98       & 63.22       \\
    \bottomrule
  \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\isPreprints{}{% This command is only used for ``preprints''.
\begin{adjustwidth}{-\extralength}{0cm}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\bibliography{references}


% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
%\isPreprints{}{% This command is only used for ``preprints''.
\end{adjustwidth}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
\end{document}

