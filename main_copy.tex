%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[bioengineering,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aieduc, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefcommunication, briefreport, casereport, changes, clinicopathologicalchallenge, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, creative, datadescriptor, discussion, entry, expressionofconcern, extendedabstract, editorial, essay, erratum, fieldguide, hypothesis, interestingimages, letter, meetingreport, monograph, newbookreceived, obituary, opinion, proceedingpaper, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, supfile, systematicreview, technicalnote, viewpoint, guidelines, registeredreport, tutorial,  giantsinurology, urologyaroundtheworld
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

% Additional packages
\usepackage{algorithm}        % アルゴリズム環境
\usepackage{algorithmic}      % アルゴリズム記述
\usepackage{pdflscape}        % 横向きページ
\usepackage[markup=underlined]{changes} % 変更履歴

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Feasibility Study of CLIP-Based Key Slice Selection in CT Images and Performance Enhancement via Lesion- and Organ-Aware Fine-Tuning}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Feasibility Study of CLIP-Based Key Slice Selection in CT Images and Performance Enhancement via Lesion- and Organ-Aware Fine-Tuning}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0009-0008-6043-4069} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0002-4222-4569} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Kohei Yamamoto $^{1}$*\orcidA{} and Tomohiro Kikuchi $^{2}$\orcidB{}}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Kohei Yamamoto, Tomohiro Kikuchi}

% MDPI internal command: Authors, for citation in the left column, only choose below one of them according to the journal style
% If this is a Chicago style journal 
% (arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci): 
% Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% If this is a APA style journal 
% (admsci, behavsci, businesses, econometrics, economies, education, ejihpe, games, humans, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth): 
% Lastname, F., Lastname, F., \& Lastname, F.

% If this is a ACS style journal (Except for the above Chicago and APA journals, all others are in the ACS format): 
% Lastname, F.; Lastname, F.; Lastname, F.
\isAPAStyle{%
       \AuthorCitation{Yamamoto, K. \& Kikuchi, T.}
         }{%
        \isChicagoStyle{%
        \AuthorCitation{Yamamoto, Kohei; Kikuchi, Tomohiro.}
        }{
        \AuthorCitation{Yamamoto, K.; Kikuchi, T.}
        }
}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Department of Radiology, School of Medicine, Jichi Medical University; yamamoto.kohei@jichi.ac.jp\\
$^{2}$ \quad Data Science Center, Jichi Medical University; r1419kt@jichi.ac.jp}

% Contact information of the corresponding author
\corres{Correspondence: yamamoto.kohei@jichi.ac.jp}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}  
% Current address should not be the same as any items in the Affiliation section.

%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes.

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{Large-scale medical visual question answering (MedVQA) datasets are critical for training and deploying vision-language models (VLMs) in radiology. Ideally, such datasets should be automatically constructed from routine radiology reports and their corresponding images. However, no existing method directly links free-text findings to the most relevant 2D slices in volumetric computed tomography (CT) scans. To address this gap, a contrastive language–image pre-training (CLIP)-based key-slice selection framework is proposed, which matches each sentence to its most informative CT slice via text-image similarity. This experiment demonstrates that models pre-trained in the medical domain already achieve competitive slice retrieval accuracy, and that fine-tuning them on a small dual-supervised dataset that imparts both lesion- and organ-level awareness yields further gains. In particular, the best-performing model (fine-tuned BiomedCLIP) achieved a Top-1 accuracy of 51.7\% for lesion-aware slice retrieval, representing a 20-point improvement over baseline CLIP, and was accepted by radiologists in 56.3\% of cases. By automating the report-to-slice alignment, the proposed method facilitates scalable, clinically realistic construction of MedVQA resources.
}

% Keywords
\keyword{CLIP; CT; key slice selection} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% 1. 現状のMedVQAの進展とその問題点
Recent advances in vision-language models (VLMs) have been driven by joint learning from paired image-text inputs \cite{li_llava-next-interleave_2024,xiao_florence-2_2024,chen_internvl_2024,alayrac_flamingo_2022}. 
Medical visual question answering (MedVQA) has become a benchmark task in this domain, requiring a model to answer questions by jointly processing a medical image and the corresponding sentence(s) \cite{lau_dataset_2018,liu_slake_2021}. 
To scale training, most studies mine figures and captions from large bibliographic databases and then employ large language models (LLMs) to synthesize visual question answering (VQA) pairs \cite{zhang_pmc-vqa_2024,li_llava-med_2023}. 
Although effective, these bibliography-based datasets overrepresent prototypical cases and often fail to capture the heterogeneity of real-world clinical presentations, potentially reducing the model performance in practice \cite{zhang_pmc-vqa_2024,dong_generative_2025}. 
Consequently, constructing MedVQA datasets directly from routine clinical data is a critical next step and could even surpass the scale of bibliography-based collections. 
Although a VQA dataset has been curated from routine chest radiograph reports, no comparable resource currently exists for volumetric modalities such as computed tomography (CT) or magnetic resonance imaging (MRI) \cite{bae_ehrxqa_2024}. 
A major barrier is that clinical reports are not annotated to show which sentence corresponds to which slice, and automated tools for establishing these links remain underdeveloped, hindering the construction of large, clinically sourced MedVQA datasets. 
Some studies have explored VLMs that use 3D encoders that pair a full image volume with the entire report; however, these models require substantial computational resources, and many state-of-the-art (SOTA) VLMs still operate on 2D inputs \cite{bai_m3d_2024,blankemeier_merlin_2024,hamamci_ct2rep_2024}. 
Moreover, everyday radiology communication—whether in report snapshots, electronic medical record attachments, or conference presentations—continues to rely on a limited set of representative 2D slices; thus, slice-level tasks remain indispensable in clinical practice. 
 
An automated key-slice selector that links each report sentence to the 2D slice that best matches its described finding is therefore essential for building the datasets needed to advance clinically useful VLMs in radiology.
The key motivation of this work is to establish such a method, bridging report sentences and CT slices to enable scalable construction of clinically realistic MedVQA datasets at this scale.

% Fig. 1
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./figures/figure1_9.png}
  \caption{
    Overview of the proposed contrastive language-image pre-training (CLIP)-based slice-selection pipeline. 
    Each report sentence is encoded by the text encoder, and each CT slice by the image encoder. 
    Cosine similarities are computed between the sentence embedding and all slice embeddings, and the slice with the highest score is chosen as the key slice for that finding. 
}
  \label{fig:clip_overview}
\end{figure}


% 2. 本研究で提案するSlice Selectorの概要
This study aims to demonstrate the utility of a slice-selection framework that aligns report sentences with CT slices and to investigate whether fine-tuning with lesion- and organ-aware supervision can further enhance performance.
Contrastive language-image pre-training (CLIP) jointly embeds images and text into a shared latent space and computes cosine similarity between these embeddings to enable zero-shot classiﬁcation and text-image retrieval \cite{radford_learning_2021}. 
We hypothesized that the same framework could be adapted to align individual report sentences with their corresponding CT slices. 
Each finding sentence was encoded by the text encoder, and every axial slice in the CT volume was encoded by the image encoder; the slice with the highest cosine similarity was selected as the key image for that sentence (Fig. \ref{fig:clip_overview}). 
This pipeline was evaluated using the original CLIP (ViT-B/16) and two medical-domain variants, PubMedCLIP and BiomedCLIP \cite{radford_learning_2021,eslami_pubmedclip_2023,zhang_biomedclip_2025}. 
In addition, we curated a compact training set from a limited number of clinical CT studies and fine-tuned each model using this dataset. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Related Works}
\subsection{MedVQA Dataset Construction}
VQA-RAD is a foundational MedVQA benchmark for radiological imaging. 
Expert radiologists manually authored clinical questions and provided the corresponding ground-truth answers, ensuring high annotation quality but resulting in a relatively small dataset \cite{lau_dataset_2018}. 
SLAKE builds on this by adding spatial annotations, such as pixel-level masks and bounding boxes, and by including knowledge-base-driven questions to probe deeper clinical reasoning \cite{liu_slake_2021}.
The effective training of VLMs in highly specialized medical domains requires much larger and more diverse datasets. 
PMC-VQA constructs a 227-K example VQA dataset, predominantly composed of radiological images, using literature extracted from PubMed Central \cite{zhang_pmc-vqa_2024}.
This dataset was built by mining images and their associated captions from articles and employing ChatGPT to generate the corresponding VQA pairs.
LLava-Med adopts a similar strategy: it first trains 600K image-caption pairs mined from PubMed Central articles and then fine-tunes the model with 60K GPT-4-generated instruction-tuning examples \cite{li_llava-med_2023}.
However, none of these datasets provide explicit links between report sentences and specific CT slices, which is the key challenge this study addresses.

% Extraction of slices from Volumetric Dataとか？
\subsection{Slice Selection}
Vote-MI was proposed as a weakly supervised method for identifying representative key slices from CT volumes \cite{wang_enhancing_2024}. Although effective in highlighting diagnostically relevant slices, Vote-MI is designed for slice selection alone and does not provide a mechanism for sentence-level text-image retrieval. This limits its applicability to tasks such as MedVQA dataset construction, where fine-grained alignment between report sentences and CT slices is required. In contrast, our method explicitly addresses this gap by enabling text-driven slice retrieval and further improving alignment performance through lesion- and organ-aware fine-tuning.

Although no prior studies have applied CLIP to slice selection, related methods for extracting keyframes from sequential medical data have been employed for zero-shot surgical phase recognition \cite{yuan_hecvl_2025}.


% CLIPの説明
\subsection{Multi Modal Learning}
With the advent of CLIP, contrastive learning has become the predominant paradigm in multimodal representation learning \cite{radford_learning_2021}. 
CLIP projects textual and visual inputs into a shared embedding space by encoding each modality separately—text via a transformer-based text encoder and images via a vision transformer—and then aligns them through cosine similarity. 
This cross-modal alignment capability has led many researchers to employ CLIP image encoders as the visual backbone in VLMs. 
Subsequent studies extended this foundation to better support downstream tasks. For example, GLIP and RegionCLIP introduce region-level contrastive objectives that improve object detection and semantic segmentation performance \cite{li_grounded_2022,zhong_regionclip_2022}.
SigLIP replaces the standard contrastive loss with a sigmoid cross-entropy formulation, substantially reducing the text-encoder context length while maintaining high-efficiency multimodal learning \cite{zhai_sigmoid_2023}. 
CLIP-Adapter enhances few-shot transfer by inserting lightweight adapter modules into a frozen CLIP backbone, thereby boosting performance on downstream tasks without extensive retraining \cite{gao_clip-adapter_2024}. 
In the medical domain, PubMedCLIP fine-tunes CLIP using ROCO, a radiology-focused corpus derived from PubMed articles \cite{eslami_pubmedclip_2023,pelka_radiology_2018}. 
BiomedCLIP expands this approach by assembling a PMC-15M dataset from PubMed Central publications and applying it to further fine-tune CLIP \cite{zhang_biomedclip_2025}. 
These biomedical variants have achieved SOTA performance in cross-modal retrieval, zero-shot image classification, and MedVQA tasks, thereby establishing themselves as foundation models for a broad range of downstream medical AI applications \cite{koleilat_medclip-sam_2024,zhao_foundation_2024,polis_exploring_2025}.


In summary, while previous research has advanced MedVQA dataset construction, slice retrieval, and multimodal learning, none has proposed an automated framework that aligns report sentences with CT slices and enhances performance through lesion- and organ-aware fine-tuning. This constitutes the novelty of the present study.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Materials and Methods}

\subsection{Dataset}\label{sec:dataset}
% データセット概要
This study was approved by Jichi Medical University Hospital Bioethics Committee for Clinical Research. 
We assembled our dataset from 137 consecutive patients with gastrointestinal cancer who underwent their ﬁrst CT examination at our institution between 2021 and 2023. 
A board-certified radiologist (Radiologist 1) with more than ten years of experience reviewed each report and selected the appropriate CT series (e.g., optimal contrast phase) for each sentence describing an abnormality. 
The radiologist then identified the CT slice that best matched the sentence and annotated the lesion with a bounding box. 
The dataset was divided into 92 training, 23 validation, and 22 test studies (Fig. \ref{fig:dataset_detail}). 
For the test set, the radiologist annotated the range of slices corresponding to each sentence to enable more detailed evaluation. 
This yielded 625 sentence-slice pairs for training, 152 for validation, and 120 for testing. 
The original reports were written in Japanese; after confirming that they contained no protected health information, all sentences were translated into English using \texttt{GPT-4o-mini} \cite{openai_gpt-4_2024}. 


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./figures/dataset_detail_1.png}
  \caption{
    Overview of dataset composition and evaluation measures.
  }
  \label{fig:dataset_detail}
\end{figure}


% totalsegmentatorによるデータ拡張手法
To improve performance on negative findings, we augmented the dataset with synthetic finding–image pairs containing no abnormalities. 
We first applied the TotalSegmentator to each CT series to extract organ masks and determine which of the 15 major thoracoabdominal organs were present in each slice \cite{wasserthal_totalsegmentator_2023}. 
Using these organ labels, we generated pseudo-findings in two ways: (1) inserting organ names into a fixed template and (2) prompting \texttt{GPT-4o-mini} to produce natural language descriptions based on the same templates. 
Further details of the rule- and LLM-based prompt designs are provided in Appendix \ref{app:rule_based_text_prompt}.


% 最終的な学習データセット
For lesion-positive examples, we expanded the slice range based on lesion size. Bounding boxes larger than 2,000 pixels were padded by one slice above and below, whereas boxes exceeding 4,000 and 6,000 pixels were expanded by two and three slices, respectively. 
This procedure yielded a training set composed of 12,743 CT slices, including 1,009 lesion-positive image pairs and 140,761 normal anatomy pairs generated from organ labels.
As part of image preprocessing, we applied soft-tissue windowing to the CT values to enhance contrast in the relevant intensity ranges. 
Because the vision transformer (ViT) backbone requires 224 × 224 inputs, we first cropped 32 pixels from each edge of the original 512 × 512 slices to obtain 448 × 448 images and then downsampled them to 224 × 224 pixels. 
During training, we applied data augmentation, such as horizontal and vertical flips, translations, scaling, rotations, elastic distortions, and cutouts, to improve model robustness \cite{devries_improved_2017}.


\subsection{Slice-Selection Algorithm}  

  To implement this study’s approach, we defined a slice-selection algorithm that links each report sentence to the most relevant CT slice. 
  As illustrated in Fig. \ref{fig:clip_overview}, the text encoder embeds the finding sentence, while the image encoder embeds each axial slice of the CT volume. 
  Cosine similarities are then computed between the sentence embedding and all slice embeddings. 
  The slice with the highest similarity score is designated as the key slice corresponding to the finding. 
  This procedure, summarized in Alg. \ref{alg:key_slice_selection}, constitutes the inference pipeline of this research method and serves as the foundation for both evaluation and fine-tuning.



\begin{algorithm}
  \caption{CLIP-based Key-Slice Selection}
  \label{alg:key_slice_selection}
  \begin{algorithmic}[1]
    \REQUIRE finding sentence $f$, CT volume $V = \{s_1, \dots, s_N\}$, text encoder $E_{\mathrm{text}}$, image encoder $E_{\mathrm{img}}$
    \ENSURE key slice $s_k$
    \STATE $e_{\mathrm{text}} \leftarrow E_{\mathrm{text}}(f)$ 
    \FOR{$i \leftarrow 1$ to $N$}
      \STATE $e_{\mathrm{img}} \leftarrow E_{\mathrm{img}}(s_i)$ 
      \STATE $\mathrm{sim}_i \leftarrow \cos\bigl(e_{\mathrm{text}}, e_{\mathrm{img}}\bigr)$ 
    \ENDFOR
    \STATE $k \leftarrow \arg\max_i \mathrm{sim}_i$ 
    \RETURN $s_k$
  \end{algorithmic}
\end{algorithm}


% 学習手法
\subsection{Models and Training Details}
We trained CLIP to learn the correspondence between report sentences and CT slices. 
For the image encoder, we used \texttt{ViT-B/16} and fine-tuned the pretrained model with the official OpenAI weights. 
The same procedure was applied to PubMedCLIP and BiomedCLIP. We summarize the image encoders and text encoders used in each method in Table \ref{tab:model_summary}.
For all text encoders, the context length was fixed at 77 tokens, and sentences exceeding this length were truncated.
All training was conducted on a single NVIDIA RTX 6000 Ada GPU with a batch size of 64 using the AdamW optimizer \cite{loshchilov_fixing_2017}. 
The learning rate was scheduled from 5e-5 to 1e-6 over 20 epochs using cosine annealing. 
The checkpoint with the lowest validation loss was selected for evaluation.
The experiments were run on Ubuntu 22.04.5 LTS with Python~3.11.9 (conda-forge), using \texttt{numpy}~2.1.1, \texttt{torch}~2.4.1+cu118, and \texttt{TotalSegmentator}~2.10.0.

\begin{table}[ht]
  \centering
  \caption{Summary of image and text encoders used in each method.}
  \label{tab:model_summary}
  \begin{tabular}{lcc}
    \toprule
    Method                  & Image Encoder      & Text Encoder      \\
    \midrule
    CLIP                    & ViT-B/16          & Transformer       \\
    PubMedCLIP              & ViT-B/32          & Transformer       \\
    BiomedCLIP              & ViT-B/16          & PubMedBERT        \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

% セクション全体の説明
We evaluated the slice-selection performance of six models: pretrained CLIP, PubMedCLIP, and BiomedCLIP, along with their fine-tuned counterparts (FT). 

  Before reporting task-specific retrieval metrics, we first verified whether each model learned meaningful alignments during training. To this end, we measured the mean absolute error (MAE) between the predicted and ground-truth slice indices using two strategies: hard prediction (selecting the top-1 slice) and soft prediction (using a local moving average).

In Section \ref{sec:lesion_aware}, we assess the retrieval accuracy of sentences describing abnormal findings from diagnostic reports. 
In addition, we report on a radiologist-led evaluation of slice selection results to examine clinical applicability.
Section \ref{sec:organ_aware} presents the alignment fidelity between organ-related text and its corresponding CT slices. 
Finally, Section \ref{sec:visualization} provides visualizations of the slice-selection results for key series, highlighting both quantitative performance and qualitative interpretability.

\subsection{Training Verification via Slice-Level MAE}

  To validate the training process, we measured the mean absolute error (MAE) between the predicted and ground-truth slice indices across all models using the image-sentence pairs from the validation set. 
  We compared two strategies: (i) hard prediction, which directly selects the slice with the highest similarity score, and (ii) soft prediction, which applies a moving average over $\pm 2$ neighboring slices to leverage the sequential continuity of CT volumes. 
  As shown in Fig. \ref{fig:mae}, fine-tuned models consistently achieved lower MAE than their pre-trained counterparts, confirming that meaningful learning occurred. 
  Moreover, the soft prediction strategy yielded fewer errors than the hard prediction strategy, motivating its adoption for all subsequent evaluations. For example, in BiomedCLIP, the MAE improved from $10.40 \pm 13.58$ with hard prediction to $9.21 \pm 11.09$ with soft prediction, corresponding to an average gain of approximately one slice in localization accuracy.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./figures/mae.png}
  \caption{
    Comparison of mean absolute error (MAE) between predicted and ground-truth slice indices for each method. 
    Results are shown for both hard prediction (top-1 slice) and soft prediction (moving average over $\pm$2 slices).
  }
  \label{fig:mae}
\end{figure}


\subsection{Lesion Awareness}\label{sec:lesion_aware}
The models are evaluated using a test set of 22 studies containing 120 sentences with manually annotated ground-truth slice ranges. 
At inference, each sentence is encoded by the CLIP text encoder, every slice in the corresponding CT volume is processed by the image encoder, cosine similarities are computed, and the slice with the highest score is selected as the model’s key slice (Fig. \ref{fig:clip_overview}, Alg. \ref{alg:key_slice_selection}). 
To assess interobserver variability, Radiologist 2—independent of the ground-truth annotator—also selected the single best-matching slice for each sentence.
Table \ref{tab:slice_extraction_accuracy} shows the proportion of Top-1 predictions falling within the annotated ranges and the proportion of cases in which any of the Top-5 most similar slices overlapped with those ranges. 
CLIP (FT) improves the Top-1 accuracy by 20 percentage points over CLIP, demonstrating effective adaptation to the medical domain.
PubMedCLIP and BiomedCLIP began with higher baseline accuracies, which are further improved by fine-tuning. 
BiomedCLIP (FT), in particular, achieves the best performance, with a Top-1 accuracy of 51.72\% and Top-5 accuracy of 64.37\%. 
Considering that Radiologist 2 achieved a Top-1 accuracy of 78.16\%, the model accuracy of 51.72\% represents a relatively strong result.



\begin{table}[ht]
  \centering
  \caption{Comparison of slice-selection accuracy for each method. 
  Reported values are Top-1 and Top-5 accuracy[\%]. 
  For each metric, the highest score among the automated methods (excluding the Radiologist 2) is highlighted in red.}
  \label{tab:slice_extraction_accuracy}
  \begin{tabular}{lcc}
    \toprule
    Method                  & Acc.@1\,↑      & Acc.@5\,↑      \\
    \midrule
    Radiologist2            & 78.16       & -           \\
    CLIP                    & 19.54       & 44.83       \\
    CLIP(FT)                & 40.23       & 49.43       \\
    PubMedCLIP              & 29.89       & 54.02       \\
    PubMedCLIP(FT)          & 42.53       & 59.77       \\
    BiomedCLIP              & 44.83       & 60.92       \\
    BiomedCLIP(FT)          & \textcolor{red}{51.72} & \textcolor{red}{64.37} \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[ht]
  \centering
  \caption{Radiologist acceptance rates[\%] of slice selector predictions for each model.}
  \label{tab:slice_selector_adoption}
  \begin{tabular}{lc}
    \toprule
    Method                  & Acceptance Rate\,↑ \\
    \midrule
    CLIP                    & 24.14        \\
    CLIP(FT)                & 40.23        \\
    PubMedCLIP              & 37.93        \\
    PubMedCLIP(FT)          & 50.57        \\
    BiomedCLIP              & 50.57        \\
    BiomedCLIP(FT)          & 56.32        \\
    \bottomrule
  \end{tabular}
\end{table}


% \section{Radiologist Evaluation of Slice Selection Results}\label{app:doctor_evaluation}
As noted above, strict ground-truth ranges do not always align with radiologists’ subjective assessments.
To evaluate whether the slices proposed by each method would be considered acceptable in clinical practice, we conducted a qualitative assessment in which a radiologist evaluated whether to adopt each suggested slice for a given sentence. 
We developed a simple web application that displayed each finding sentence alongside the slices proposed by each slice selector. 
For each case, the suggestions from all models were presented in a random order on the same screen, and the radiologist was blinded to the source model. 
Table \ref{tab:slice_selector_adoption} reports the adoption rates for each model’s recommendations. 
Although these adoption rates generally correspond to the quantitative results presented in Table \ref{tab:slice_extraction_accuracy} (Top-1 accuracy), they are uniformly slightly higher. 
These higher adoption rates occur when lesions deemed acceptable by the radiologist fall outside the strict ground-truth range—for example, calcifications extending across several slices or annotation covering only one of multiple hepatic cysts.


\subsection{Organ Awareness}\label{sec:organ_aware}
Recognizing normal anatomy is as important as detecting abnormalities, so we deﬁne organ-aware slice selection as a complementary evaluation. 
Using TotalSegmentator, we generated binary organ-presence labels for each CT slice based on the 15 major thoracoabdominal organs \cite{wasserthal_totalsegmentator_2023}. 
For each model, we encoded the organ prompt and each CT slice, computed their cosine similarities, and then determined organ-specific thresholds using the Youden index on the validation set, which served as cutoffs for determining organ presence in each test slice \cite{youden_index_1950}. 
To assess the impact of text granularity, we compared two input types: single-word organ names (e.g., “Heart,” “Liver”) and full template sentences (e.g., “This CT image includes the heart”).
These experiments test whether word- or sentence-level prompts are more effective when querying a text encoder. 


Table \ref{tab:organ_extraction_results} summarizes the organ-aware evaluation, reporting both the accuracy and F1-score for each model under word- and sentence-prompt conditions. 
BiomedCLIP (FT) achieved the highest performance in both settings, with an F1-score of 0.85 for Word and Sentence inputs. 
PubMedCLIP showed similar accuracy and F1-scores across the two prompt types, while BiomedCLIP tended to perform better with sentence prompts. 
Fine-tuning yielded comparable gains for Word and Sentence inputs across all models, indicating that our approach generalized well to both single-token organ names and fully descriptive sentences. 
On average, fine-tuning improved the accuracy and F1-score by more than 10\% for each model. 
These substantial improvements demonstrate enhanced normal-anatomy recognition accuracy not only for the original CLIP model but also for the medical-domain pre-trained variants (PubMedCLIP and BiomedCLIP). 
They further suggest that fine-tuning improves slice extraction accuracy for non-abnormal findings, thereby benefiting downstream VQA generation.
F1-scores for each organ are provided in Appendix \ref{app:organ_aware_details}.


\begin{table}[ht]
  \centering
  \caption{Comparison of organ extraction performance. Reported metrics are accuracy (Acc.) and F1-score (F1). 
  “Word” indicates prompts using single organ names, and “Sentence” indicates prompts using full finding sentences. 
  The highest score in each column is highlighted in red.}
  \label{tab:organ_extraction_results}
  \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Word}               & \multicolumn{2}{c}{Sentence}           \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
                           & Acc.\,↑ & F1\,↑         & Acc.\,↑   & F1\,↑         \\ 
    \midrule
    CLIP                    & $0.715\pm0.105$ & $0.514\pm0.182$ & $0.725\pm0.071$ & $0.520\pm0.192$ \\
    CLIP(FT)                & $0.927\pm0.038$ & $0.806\pm0.153$ & $0.934\pm0.035$ & $0.824\pm0.129$ \\
    PubMedCLIP              & $0.881\pm0.046$ & $0.722\pm0.164$ & $0.882\pm0.042$ & $0.726\pm0.157$ \\
    PubMedCLIP(FT)          & $0.943\pm0.030$ & $0.839\pm0.123$ & $0.944\pm0.031$ & $0.843\pm0.121$ \\
    BiomedCLIP              & $0.832\pm0.133$ & $0.673\pm0.230$ & $0.884\pm0.048$ & $0.730\pm0.126$ \\
    BiomedCLIP(FT)          & \textcolor{red}{$0.947\pm0.033$} & \textcolor{red}{$0.853\pm0.118$} & \textcolor{red}{$0.948\pm0.032$} & \textcolor{red}{$0.854\pm0.118$} \\
    \bottomrule
  \end{tabular}
\end{table}



Additionally, as an external dataset benchmark, we evaluated organ awareness using the publicly available CT-RATE dataset, which consists of chest CT volumes with organ labels generated by TotalSegmentator \cite{hamamci_ct2rep_2024}. 
We evaluated 1,304 cases from the validation split, selecting the earliest series (denoted as “\_a\_1” in the dataset) when multiple CT series were available per patient. 
The results are summarized in Table \ref{tab:organ_extraction_results_ctrate}. 
Overall performance was lower than that reported in Table \ref{tab:organ_extraction_results}, likely due to domain shifts related to imaging devices and acquisition parameters. 
For example, BiomedCLIP(FT) achieved an accuracy of 0.926 and an F1-score of 0.809 on CT-RATE, compared with 0.948 and 0.854, respectively, on the in-house dataset. 
Despite this decrease, BiomedCLIP(FT) consistently yielded the highest accuracy across all models, confirming its robustness to dataset variation.



\begin{table}[ht]
  \centering
  \caption{Comparison of organ extraction performance on the external CT-RATE dataset. 
  Reported metrics are accuracy (Acc.) and F1-score (F1). 
  “Word” indicates prompts using single organ names, and “Sentence” indicates prompts using full finding sentences. 
  The highest score in each column is highlighted in red.}
  \label{tab:organ_extraction_results_ctrate}
  \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Word}               & \multicolumn{2}{c}{Sentence}           \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
                          & Acc.\,↑ & F1\,↑         & Acc.\,↑   & F1\,↑         \\ 
    \midrule
    CLIP                    & $0.726\pm0.059$ & $0.554\pm0.223$ & $0.724\pm0.065$ & $0.556\pm0.226$ \\
    CLIP(FT)                & $0.902\pm0.050$ & $0.770\pm0.251$ & $0.902\pm0.048$ & $0.770\pm0.250$ \\
    PubMedCLIP              & $0.842\pm0.099$ & $0.680\pm0.237$ & $0.861\pm0.095$ & $0.707\pm0.226$ \\
    PubMedCLIP(FT)          & $0.911\pm0.072$ & $0.796\pm0.252$ & $0.910\pm0.073$ & $0.795\pm0.252$ \\
    BiomedCLIP              & $0.749\pm0.171$ & $0.632\pm0.271$ & $0.755\pm0.125$ & $0.604\pm0.226$ \\
    BiomedCLIP(FT)          & \textcolor{red}{$0.926\pm0.059$} & \textcolor{red}{$0.809\pm0.257$} & \textcolor{red}{$0.926\pm0.059$} & \textcolor{red}{$0.809\pm0.257$} \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Visualization}\label{sec:visualization}
Fig. \ref{fig:lesion_aware_results} shows an example of a lesion-aware slice-selection result on the test cohort. 
For uterine fibroids, PubMedCLIP, BiomedCLIP, and BiomedCLIP(FT) correctly identified key CT slices containing the lesion.
Fig. \ref{fig:organ_aware_results} shows similarity profiles between organ-related text and CT slices for a selected series. 
The original CLIP model exhibited numerous false positives and inconsistent alignments, whereas CLIP (FT) produced a much cleaner correspondence and improved localization. 
Although PubMedCLIP and BiomedCLIP also showed false positives and negatives, both models achieved high accuracy in recognizing the target organ after fine-tuning. 
Additional qualitative examples are presented in Appendix \ref{app:visualization}.


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/figure2_4.png}
  \caption{"Multiple masses that may represent fibroids are observed in the myometrium and beneath the endometrium."
  The Ground Truth column shows the reference CT slice with the radiologist-annotated lesion bounding box.
  The other columns display each model’s key-slice prediction.
  The white double circle symbol indicates predictions that match the ground-truth slice range.
  }
  \label{fig:lesion_aware_results}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/figure3_4.png}
  \caption{Similarity profile between the prompt “Heart” and each slice. 
  The green line denotes the ground-truth slice range. 
  Similarity profiles increase from left to right, and the dashed line indicates the decision threshold. 
  }
  \label{fig:organ_aware_results}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

In this study, we introduced a CLIP-based key-slice selector that aligns free-text radiology findings with corresponding CT slices. 
The approach was evaluated along two complementary axes: lesion and organ awareness. 
Despite training on a relatively small dataset, the fine-tuned models achieved substantial gains in lesion-aware slice retrieval, improving accuracy by 7–20 percentage points over the pre-trained baselines. 
The best performer, BiomedCLIP (FT), achieved 51\% Top-1 accuracy for lesion localization. 
In the organ-aware task, all fine-tuned models achieved improvements of over 10 \% in both accuracy and F1-score. 
Notably, the performance remained strong regardless of whether organ prompts were provided as single words or full sentences, demonstrating the flexibility of the text-encoding strategy. 

  To further examine the robustness under external conditions, we conducted an additional organ-awareness evaluation on the publicly available CT-RATE dataset. 
  Although all methods exhibited some degradation in performance owing to domain shifts in imaging devices and acquisition protocols, the decrease was relatively modest, and BiomedCLIP (FT) consistently achieved the highest accuracy. 
  These findings suggest that this study’s approach is robust across datasets and can be generalized beyond the in-house cohort.



  Regarding system requirements, all experiments were conducted on a single NVIDIA RTX 6000 Ada GPU with 48 GB of memory, which was sufficient to train models with a batch size of 64. 
  Each training run with 20 epochs finished within one hour. 
  These results indicate that the proposed method can be reproduced on a high-end single-GPU workstation without requiring distributed resources.



  Beyond the dataset used in this study, the proposed slice-selection framework offers practical benefits when applied to diverse imaging archives. 
  First, automatically aligning report sentences with key slices can substantially reduce the cost of two-dimensional (2D) slice-level annotation, which is often a major bottleneck in dataset curation. 
  Second, it enables the construction of image-text pairs directly from unlabeled CT volumes and routine reports, providing a low-cost pathway for expanding training resources for medical VLMs. 
  These improvements highlight the broader utility of the proposed method across datasets and institutions, extending its impact beyond the present feasibility study.


The method paves the way for fully automated construction of MedVQA datasets from routine clinical archives, directly addressing the biases and scale limitations inherent in bibliographic collections. 
Furthermore, the radiologist’s evaluation of the slice selection results (Table \ref{tab:slice_selector_adoption}) and visualization of similarity profiles (Fig. \ref{fig:organ_aware_results}) suggest that, in addition to serving as a VQA dataset generator, integrating the slice selector into clinical workflows as a recommendation tool could help reduce radiologists’ workload and enhance diagnostic accuracy. 


\textbf{Limitations}
First, as this was a feasibility study, the training and test cohorts were small and drawn from a single institution. Expanding the dataset size could uncover additional gains in retrieval accuracy and reduce the risk of overﬁtting to the relatively small evaluation cohorts. 
Second, the evaluation reports only overall lesion- and organ-aware metrics; it does not stratify the results by lesion size, anatomical region, contrast phase, or disease category. Finer-grained analysis is essential to uncover failure modes in clinically relevant subgroups. 
Third, the Youden-index thresholds used to determine organ presence were chosen from the validation set and fixed for testing. Although this mirrors a real-world deployment scenario, alternative calibration strategies may yield better threshold stability for unseen data. 


\textbf{Future Work}
In future work, we will integrate a key-slice selector into a complete MedVQA-generation pipeline consisting of four main stages. 
\begin{enumerate}
  \item segmenting diagnostic reports into individual finding units, 
  \item acquiring the corresponding imaging series (including selection of contrast phases),
  \item extracting key slices using the slice selector, and 
  \item generating VQA pairs from each finding sentence through rule-based or LLMs.
\end{enumerate}
While this study addresses Stage 3, the remaining steps are critical for end-to-end automation.
Accordingly, we plan to develop and evaluate methods for report segmentation and image retrieval, and to implement VQA generation using both deterministic templates and generative LLM prompts. 
As these components mature, we will iteratively expand the clinical dataset and refine the slice selector’s accuracy. 

  We also plan to train VLMs on the resulting automatically constructed MedVQA dataset and compare their performance with models trained on manually curated slice annotations to quantify the impact of automated slice selection on the downstream VLM capabilities.



  Beyond dataset construction, architectural advances offer promising directions. 
  Recent large-scale VLMs such as Merlin, which align 3D CT volumes with radiology reports through contrastive learning, provide useful design principles \cite{blankemeier_merlin_2024}. 
  Incorporating similar volume-level representation strategies, as well as techniques from the CLIP-Driven Universal Model—which uses text embeddings as semantic labels for segmentation and detection—could extend the framework beyond slice retrieval to fine-grained localization and more efficient MedVQA dataset construction \cite{10376801}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

This study introduced a CLIP-based key-slice selector that aligns free-text radiology findings with the corresponding CT images. Fine-tuning on a compact dual-supervised dataset substantially improved lesion-aware accuracy, with the best-performing model achieving strong results in lesion localization. In the organ-aware task, all fine-tuned models demonstrated gains in both accuracy and F1-score, regardless of whether the prompts were provided as single words or full sentences. A qualitative review indicated that the slice selector’s recommendations could reduce radiologists’ workload and enhance diagnostic confidence in routine practice. Nevertheless, the current evaluation is limited by its single-institution, small-cohort design, lack of stratification by lesion size or disease type, and reliance on fixed threshold calibration, all of which limit generalizability. Future work will expand the dataset, incorporate stratified analyses, and integrate the selector into an end-to-end MedVQA pipeline encompassing report segmentation, series retrieval, slice extraction, and automatic question-answer generation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following supporting information can be downloaded at:  \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.}

% Only for journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title. A supporting video article is available at doi: link.}

% Only used for preprtints:
% \supplementary{The following supporting information can be downloaded at the website of this paper posted on \href{https://www.preprints.org/}{Preprints.org}.}

% Only for journal Hardware:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.\vspace{6pt}\\
%\begin{tabularx}{\textwidth}{lll}
%\toprule
%\textbf{Name} & \textbf{Type} & \textbf{Description} \\
%\midrule
%S1 & Python script (.py) & Script of python source code used in XX \\
%S2 & Text (.txt) & Script of modelling code used to make Figure X \\
%S3 & Text (.txt) & Raw data from experiment X \\
%S4 & Video (.mp4) & Video demonstrating the hardware in use \\
%... & ... & ... \\
%\bottomrule
%\end{tabularx}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, K.Y. and T.K.; methodology, K.Y.; software, K.Y.; validation, K.Y.; formal analysis, K.Y.; investigation, K.Y. and T.K.; resources, T.K.; data curation, K.Y. and T.K.; writing---original draft preparation, K.Y.; writing---review and editing, T.K.; visualization, K.Y.; supervision, T.K.; project administration, K.Y. All authors have read and agreed to the published version of the manuscript.}

\funding{This research received no external funding.}

\institutionalreview{The study was conducted in accordance with the Declaration of Helsinki, and approved by the Jichi Medical University Hospital Bioethics Committee for Clinical Research (protocol code 24-061 and date of approval 25 September 2024).}

\informedconsent{This retrospective study was approved by the Jichi Medical University Hospital Bioethics Committee for Clinical Research, and the requirement for informed consent was waived due to the retrospective nature of the study and the use of anonymized medical imaging data that poses minimal risk to participants.}

\dataavailability{The datasets generated and analyzed during the current study are not publicly available due to privacy and ethical restrictions related to patient medical imaging data. The source code used for the analysis is available from the corresponding author upon reasonable request.}

\conflictsofinterest{The authors declare no conflicts of interest.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional

%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

\abbreviations{Abbreviations}{
The following abbreviations are used in this manuscript:
\\

\noindent 
\begin{tabular}{@{}ll}
CLIP & Contrastive Language-Image Pre-training\\
CT & Computed Tomography\\
VLM & Vision-Language Model\\
MedVQA & Medical Visual Question Answering\\
LLM & Large Language Model\\
SOTA & State-of-the-Art\\
VQA & Visual Question Answering\\
\end{tabular}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixstart
\appendix
\section{Findings Text Generation}\label{app:rule_based_text_prompt}
The following templates were designed to generate non-abnormal finding sentences using organ labels predicted by TotalSegmentator. 
In each template, the placeholder \texttt{\{\{organ\}\}} was replaced with the corresponding organ label obtained from TotalSegmentator. 
Note that the original templates were written in Japanese and later translated into English; therefore, their word order may differ slightly from natural English syntax.
\subsection{Rule-based}
\begin{verbatim}
  organs = [
      "aorta",
      "colon",
      "duodenum",
      "esophagus",
      "gallbladder",
      "heart",
      "kidney",
      "liver",
      "pancreas",
      "stomach",
      "spleen",
      "thyroid_gland",
      "urinary_bladder",
      "trachea",
      "lung",
  ]
  text_templates = {
    "prefix": [
      "In the image,",
      "In this image,",
      "In this CT image,”,
      "",
    ],
    "suffix": [
      "{{organ}} is visible."
      "{{organ }} is included."
      "{{organ}} is contained."
      "{{organ}} etc. are visible."
      "{{organ }} etc. are included."
      "{{organ }} etc. are contained."
    ],
  }
\end{verbatim}
\subsection{LLMs-based}
We generated 20 sentence templates for each organ. Below, we present representative examples for the liver and pancreas.
\begin{verbatim}
  {
    'liver': [
      'No abnormal findings are observed in the liver, and normal morphology is maintained.',
      ''No abnormal findings are observed in the liver, \
      and morphology falls within normal limits.',
      'No lesions or abnormal findings are noted in the liver.',
      'No abnormal findings are observed in the liver, and normal morphology is maintained.',
      'No abnormal findings are observed in the liver, and it remains within normal limits.',
      'No abnormal masses or tumors can be identified in the liver.',
      'No abnormal findings are observed in the liver, and its structure is normal.',
      'No abnormal findings are observed in the liver, and its normal state is preserved.',
      'No abnormal findings are observed in the liver, \
      and its shape and size are within normal limits.',
      'No abnormal masses or fatty changes are noted in the liver.',
      'No abnormal masses or deformities are observed in the liver, \
      and findings are within normal limits.',
      'No findings indicative of abnormality are observed in the liver.',
      'No deformities or neoplastic lesions are observed in the liver, \
      which maintains normal morphology.',
      'No morphological abnormalities or neoplastic lesions are observed in the liver.',
      'No significant lesions or impairments are detected in the liver.',
      'No particular abnormalities are observed in the liver, \
      which preserves normal morphology.',
      'No abnormalities in shape or echo texture are noted in the liver.',
      'No obvious lesions or abnormal findings are noted in the liver.',
      'No abnormal masses or perfusion changes are observed in the liver.',
      'No abnormal findings are observed in the liver, and its normal structure is preserved.'
    ],
  'pancreas': [
    'No abnormal findings are observed in the pancreas.',
    'No findings suggestive of abnormality are noted in the pancreas.',
    'No significant abnormalities are observed in the pancreas.',
    'No findings indicating abnormality are apparent in the pancreas.',
    'No abnormal findings are observed in the pancreas, and normal morphology is maintained.',
    'No abnormal masses or signs of inflammation are observed in the pancreas.',
    'No masses or inflammatory findings are noted in the pancreas.',
    'No abnormal findings are observed in the pancreas, \
    and its morphology falls within normal limits.',
    'No tumors or inflammatory findings are observed in the pancreas, \
    which retains normal morphology.',
    'No findings suggestive of abnormality are apparent in the pancreas.',
    'No structural abnormalities or lesions are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas.',
    'No tumors or abnormal structural changes are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas, \
    and normal imaging appearance is obtained.',
    'No abnormal findings are observed in the pancreas, \
    and normal morphology is confirmed.',
    'No abnormal masses or inflammation are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas.',
    'No abnormal findings of any kind are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas, and normal morphology is present.',
    'No findings suggestive of lesions are observed in the pancreas.'
    ]
  }
\end{verbatim}


% \section{Radiologist Evaluation of Slice Selection Results}\label{app:doctor_evaluation}
% In addition to supporting automated MedVQA dataset creation, our slice selector can serve as a clinical decision support tool by 
% recommending the most relevant CT slices to radiologists. 
% To assess this utility, we conducted a qualitative evaluation in which a radiologist reviewed the slice suggestions generated by 
% each model for a series of finding sentences. 
% Rather than measuring strict overlap with annotated ground truth, 
% the radiologist indicated whether they would accept the recommended slice as appropriate for the given finding.
% The radiologist who participated in this evaluation was not the same individual who performed the dataset annotations. これは菊地先生だわ。
% We developed a simple web application to display each finding sentence alongside the proposed slices. 
% For each case, the suggestions from all models were presented in random order, and the radiologist was blinded to model identity. 
% Table \ref{tab:slice_selector_adoption} reports the adoption rate for each model’s recommendations, 
% defined as the percentage of suggestions the radiologist deemed acceptable.
% The adoption rates generally mirror the quantitative Top-1 accuracy results shown in Table \ref{tab:slice_extraction_accuracy}, 
% although overall rates tended to be slightly higher. 
% We attribute this to lesions such as calcifications that span broader regions: even when a suggested slice falls outside the strict ground-truth interval, 
% it may still be clinically acceptable to the radiologist.


\clearpage
\begin{landscape}


\section{Detailed Organ-Aware Results}\label{app:organ_aware_details}


\begin{table}[ht]
  \centering
  \caption{F1 scores for each organ across slice selection methods. \\
  “Word” denotes prompts using only the organ name, 
  and “Sentence” denotes prompts using full finding sentences.}
  \label{tab:organ_f1_comparison}
  \footnotesize
  \begin{tabular}{l*{6}{cc}}
    \toprule
    Organ
      & \multicolumn{2}{c}{CLIP}
      & \multicolumn{2}{c}{CLIP(FT)}
      & \multicolumn{2}{c}{PubMedCLIP}
      & \multicolumn{2}{c}{PubMedCLIP(FT)}
      & \multicolumn{2}{c}{BiomedCLIP}
      & \multicolumn{2}{c}{BiomedCLIP(FT)} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}%
    \cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}
      & word & sentence
      & word & sentence
      & word & sentence
      & word & sentence
      & word & sentence
      & word & sentence \\
    \midrule
    aorta             & 0.679 & 0.660 & 0.960 & 0.960 & 0.852 & 0.836 & 0.966 & 0.967 & 0.858 & 0.754 & 0.943 & 0.948 \\
    colon             & 0.676 & 0.747 & 0.877 & 0.877 & 0.817 & 0.859 & 0.880 & 0.878 & 0.878 & 0.886 & 0.878 & 0.878 \\
    duodenum          & 0.450 & 0.522 & 0.761 & 0.775 & 0.749 & 0.762 & 0.804 & 0.841 & 0.665 & 0.724 & 0.798 & 0.828 \\
    esophagus         & 0.614 & 0.731 & 0.932 & 0.931 & 0.936 & 0.918 & 0.958 & 0.958 & 0.873 & 0.895 & 0.973 & 0.972 \\
    gallbladder       & 0.250 & 0.235 & 0.494 & 0.495 & 0.450 & 0.472 & 0.547 & 0.547 & 0.432 & 0.451 & 0.543 & 0.537 \\
    heart             & 0.554 & 0.643 & 0.933 & 0.941 & 0.842 & 0.832 & 0.945 & 0.943 & 0.664 & 0.841 & 0.956 & 0.955 \\
    kidney            & 0.684 & 0.576 & 0.886 & 0.887 & 0.767 & 0.788 & 0.885 & 0.884 & 0.733 & 0.696 & 0.917 & 0.917 \\
    liver             & 0.600 & 0.570 & 0.900 & 0.901 & 0.791 & 0.832 & 0.928 & 0.921 & 0.821 & 0.750 & 0.914 & 0.912 \\
    pancreas          & 0.588 & 0.518 & 0.801 & 0.807 & 0.731 & 0.716 & 0.836 & 0.841 & 0.730 & 0.755 & 0.838 & 0.824 \\
    stomach           & 0.402 & 0.418 & 0.720 & 0.737 & 0.650 & 0.612 & 0.780 & 0.767 & 0.699 & 0.673 & 0.823 & 0.827 \\
    spleen            & 0.325 & 0.409 & 0.731 & 0.744 & 0.619 & 0.685 & 0.809 & 0.811 & 0.665 & 0.639 & 0.810 & 0.800 \\
    thyroid gland     & 0.363 & 0.227 & 0.744 & 0.744 & 0.552 & 0.576 & 0.648 & 0.647 & 0.219 & 0.643 & 0.727 & 0.728 \\
    urinary bladder   & 0.165 & 0.210 & 0.491 & 0.691 & 0.387 & 0.386 & 0.739 & 0.779 & 0.160 & 0.575 & 0.753 & 0.762 \\
    trachea           & 0.542 & 0.514 & 0.900 & 0.905 & 0.730 & 0.714 & 0.881 & 0.884 & 0.796 & 0.767 & 0.945 & 0.947 \\
    lung              & 0.816 & 0.827 & 0.964 & 0.963 & 0.954 & 0.904 & 0.976 & 0.975 & 0.906 & 0.907 & 0.980 & 0.981 \\
    \bottomrule
  \end{tabular}
\end{table}
\end{landscape}


\section{Visualization}\label{app:visualization}
\subsection{Lesion Awareness}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst6_2.png}
  \caption{Visualization of lesion-aware slice selection for the finding: "Circumferential wall thickening is observed in the antrum to pylorus of the stomach, which is consistent with findings of gastric cancer. 
  There is evidence of extramural invasion."
  The Ground Truth column shows the reference CT slice with the radiologist-annotated lesion bounding box.
  The other columns display each model’s key-slice prediction.
  The white double circle symbol indicates predictions that match the ground-truth slice range.}
  \label{fig:lesion_aware_inst6}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst7_2.png}
  \caption{Visualization of lesion-aware slice selection for the finding: "Multiple enlarged lymph nodes are observed in the celiac artery region, which is considered metastatic."
  In this case, no prediction fell within the ground-truth range.}
  \label{fig:lesion_aware_inst7}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst8_2.png}
  \caption{Visualization of lesion-aware slice selection for the finding: 
"Ring enhancement lesions are observed in both lobes of the liver, indicating the presence of hepatic metastases."}
  \label{fig:lesion_aware_inst8}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst10_2.png}
  \caption{Visualization of lesion-aware slice selection for the finding: "Prostate enlargement is observed."}
  \label{fig:lesion_aware_inst10}
\end{figure}


\clearpage


\subsection{Organ Awareness}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_organ_aware_liver_2.png}
  \caption{Similarity profile between the prompt “Liver” and each axial CT slice. 
  The green line denotes the ground-truth slice range. 
  The dashed line indicates the Youden-index threshold determined on the validation set.}
  \label{fig:organ_aware_liver}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_organ_aware_kidney_2.png}
  \caption{Similarity profile between the prompt “Kidney” and each axial CT slice.}
  \label{fig:organ_aware_kidney}
\end{figure}

\clearpage

\section{Impact of Organ-Aware Synthetic Data}\label{app:impact}

  To evaluate the contribution of synthetic data generated from organ labels, we conducted an ablation study in which the models were fine-tuned only on report-derived lesion sentences, without using any organ-aware synthetic data. 
  Because the training set in this condition contained only 625 sentence-slice pairs (Fig. \ref{fig:dataset_detail}), we adopted a milder fine-tuning schedule with the learning rate decayed from 1e-5 to 1e-6 over 20 epochs. 
  This model was referred to as BiomedCLIP(lesion-FT). 
  Table \ref{tab:impact} compares the lesion-awareness accuracies of BiomedCLIP, BiomedCLIP(lesion-FT), and BiomedCLIP(FT).
  The results show that BiomedCLIP(lesion-FT) achieved a higher Top-1 accuracy than the original BiomedCLIP baseline (45.98\% vs. 44.83\%), but remained inferior to BiomedCLIP(FT), which incorporated organ-aware synthetic data (51.72\%).
  These findings indicate that synthetic data are beneficial not only for organ-aware evaluation, but also for improving lesion-aware performance.


\begin{table}[ht]
  \centering
  \caption{Lesion-aware accuracy [\%] of BiomedCLIP variants. BiomedCLIP(lesion-FT) was fine-tuned only on lesion sentences without synthetic data.}
  \label{tab:impact}
  \begin{tabular}{lcc}
    \toprule
    Method                  & Acc.@1\,↑      & Acc.@5\,↑      \\
    \midrule
    BiomedCLIP              & 44.83       & 60.92       \\
    BiomedCLIP(FT)          & 51.72       & 64.37       \\
    BiomedCLIP(lesion-FT)   & 45.98       & 63.22       \\
    \bottomrule
  \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\isPreprints{}{% This command is only used for ``preprints''.
\begin{adjustwidth}{-\extralength}{0cm}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\bibliography{references}


% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
%\isPreprints{}{% This command is only used for ``preprints''.
\end{adjustwidth}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
\end{document}


