%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[bioengineering,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aieduc, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefcommunication, briefreport, casereport, changes, clinicopathologicalchallenge, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, creative, datadescriptor, discussion, entry, expressionofconcern, extendedabstract, editorial, essay, erratum, fieldguide, hypothesis, interestingimages, letter, meetingreport, monograph, newbookreceived, obituary, opinion, proceedingpaper, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, supfile, systematicreview, technicalnote, viewpoint, guidelines, registeredreport, tutorial,  giantsinurology, urologyaroundtheworld
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

% Additional packages
\usepackage{algorithm}        % アルゴリズム環境
\usepackage{algorithmic}      % アルゴリズム記述
\usepackage{pdflscape}        % 横向きページ
\usepackage[markup=underlined]{changes} % 変更履歴

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Feasibility Study of CLIP-Based Key Slice Selection in CT Images and Performance Enhancement via Lesion- and Organ-Aware Fine-Tuning}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Feasibility Study of CLIP-Based Key Slice Selection in CT Images and Performance Enhancement via Lesion- and Organ-Aware Fine-Tuning}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0009-0008-6043-4069} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0002-4222-4569} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Kohei Yamamoto $^{1}$*\orcidA{} and Tomohiro Kikuchi $^{2}$\orcidB{}}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Kohei Yamamoto, Tomohiro Kikuchi}

% MDPI internal command: Authors, for citation in the left column, only choose below one of them according to the journal style
% If this is a Chicago style journal 
% (arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci): 
% Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% If this is a APA style journal 
% (admsci, behavsci, businesses, econometrics, economies, education, ejihpe, games, humans, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth): 
% Lastname, F., Lastname, F., & Lastname, F.

% If this is a ACS style journal (Except for the above Chicago and APA journals, all others are in the ACS format): 
% Lastname, F.; Lastname, F.; Lastname, F.
\isAPAStyle{%
       \AuthorCitation{Yamamoto, K. & Kikuchi, T.}
         }{%
        \isChicagoStyle{%
        \AuthorCitation{Yamamoto, Kohei; Kikuchi, Tomohiro.}
        }{
        \AuthorCitation{Yamamoto, K.; Kikuchi, T.}
        }
}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Department of Radiology, School of Medicine, Jichi Medical University; yamamoto.kohei@jichi.ac.jp
$^{2}$ \quad Data Science Center, Jichi Medical University; r1419kt@jichi.ac.jp}

% Contact information of the corresponding author
\corres{Correspondence: yamamoto.kohei@jichi.ac.jp}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}  
% Current address should not be the same as any items in the Affiliation section.

%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes.

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. ) 
\abstract{Large-scale medical visual question answering (MedVQA) datasets are critical for training and deploying vision-language models (VLMs) in radiology. Ideally, such datasets \replaced{should}{would} be \replaced{automatically}{mined} \replaced{constructed}{automatically} from routine radiology reports and their \replaced{corresponding}{accompanying} \replaced{images.}{images,} \replaced{However,}{yet} no existing method\added{ directly} links free-text findings to the \added{most }relevant 2D slices in volumetric \added{computed tomography (CT) }scans. \replaced{To}{We} \replaced{address}{bridge} this \replaced{gap,}{gap with} a \replaced{contrastive language–image pre-training (CLIP)-based}{CLIP-based} key-slice \replaced{selection}{selector} \replaced{framework is proposed, which}{that} matches each\deleted{ finding} sentence to its most informative CT slice via text-image similarity. \replaced{This}{Our} \replaced{experiment}{experiments} \replaced{demonstrates}{show} that models pre-trained \replaced{in}{on} the medical domain already \replaced{achieve}{provide} \replaced{competitive}{relatively} \replaced{slice}{good} \replaced{retrieval}{slice-retrieval} accuracy, and that fine-tuning them on a small dual-supervised dataset that imparts both lesion- and organ-level awareness yields further gains. In particular, the best-performing model \replaced{(fine-tuned}{(BiomedCLIP} \replaced{BiomedCLIP)}{fine-tuned)} achieved a Top-1 accuracy of 51.7\% for lesion-aware slice retrieval,\added{ representing} a 20-point improvement over baseline CLIP, and was accepted by radiologists in 56.3\% of cases. By automating \added{the }report-to-slice alignment, the proposed method \replaced{facilitates}{enables} scalable, \replaced{clinically realistic}{clinical-data-driven} construction of MedVQA resources.
}

% Keywords
\keyword{CLIP; CT; key slice selection} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% 1. 現状のMedVQAの進展とその問題点
Recent advances in vision-language models (VLMs) have been driven by joint learning from paired image-text inputs \cite{li_llava-next-interleave_2024,xiao_florence-2_2024,chen_internvl_2024,alayrac_flamingo_2022}. 
\replaced{Medical}{In medical VLM research, medical} visual question answering (MedVQA) \replaced{has}{is} \replaced{become}{the} \replaced{a}{de-facto} benchmark \replaced{task}{task,} \replaced{in this domain, requiring}{where} a model \replaced{to answer}{answers} questions by jointly processing a medical image and the corresponding \deleted{findings }sentence(s) \cite{lau_dataset_2018,liu_slake_2021}. 
To scale training, most \replaced{studies}{work} \replaced{mine}{mines} figures and captions from large bibliographic databases and then \replaced{employ}{uses} large language models (LLMs) to synthesize \replaced{visual question answering (VQA)}{VQA} pairs \cite{zhang_pmc-vqa_2024,li_llava-med_2023}. 
\replaced{Although}{While} effective, these \replaced{bibliography-based}{bibliographic-based} datasets \replaced{overrepresent}{over-represent} prototypical cases and \replaced{often}{may} fail to capture the heterogeneity of real-world clinical presentations, potentially \replaced{reducing the}{degrading} model performance in practice \cite{zhang_pmc-vqa_2024,dong_generative_2025}. 
Consequently, constructing MedVQA datasets directly from routine clinical data \replaced{is}{would} \replaced{a}{bridge} \replaced{critical}{this} \replaced{next}{realism} \replaced{step}{gap} and could even \replaced{surpass}{exceed} the scale of \replaced{bibliography-based}{bibliographic-based} collections. 
Although a VQA dataset has been curated from routine \replaced{chest radiograph}{chest-radiograph} reports, no comparable resource \replaced{currently}{has} \replaced{exists}{yet been created} for volumetric modalities such as \replaced{computed tomography (CT)}{CT} or \replaced{magnetic resonance imaging (MRI)}{MRI} \cite{bae_ehrxqa_2024}. 
\replaced{A}{One} \replaced{major}{key} \replaced{barrier}{reason} is that\added{ clinical} reports are not annotated to show which sentence \replaced{corresponds}{belongs} to which slice, and \deleted{we still lack }automated tools \replaced{for}{to} \replaced{establishing}{add} \replaced{these}{those} \replaced{links}{links,} \replaced{remain}{making} \replaced{underdeveloped,}{it} \replaced{hindering}{hard} \replaced{the}{to} \replaced{construction of}{build} large, clinically sourced MedVQA datasets. 
Some studies have \replaced{explored}{experimented with} VLMs that use 3D \replaced{encoders}{encoders,} \replaced{that pair}{pairing} a full image volume with the entire report; however, these models require \replaced{substantial}{heavy} computational resources, and many state-of-the-art (SOTA) VLMs still operate on 2D inputs \cite{bai_m3d_2024,blankemeier_merlin_2024,hamamci_ct2rep_2024}. 
Moreover, everyday radiology communication—whether in report snapshots, electronic medical record attachments, or conference \replaced{presentations—continues}{slides—still} \replaced{to}{revolves} \replaced{rely}{around} \replaced{on}{just} a \replaced{limited}{few} \replaced{set of representative}{key} 2D \replaced{slices;}{images,} \replaced{thus,}{so} slice-level tasks\deleted{ will} remain indispensable in clinical practice. 
 
An automated key-slice selector that links each report sentence to the 2D slice that best matches its described finding is therefore essential for building the datasets needed to advance clinically useful VLMs in radiology.
The key motivation of this work is to establish such a method, bridging report sentences and CT slices to \replaced{enable}{make} \replaced{scalable}{the} construction of clinically realistic MedVQA datasets \replaced{at}{feasible} \replaced{this}{at} scale.

% Fig. 1
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./figures/figure1_9.png}
  \caption{
    Overview of \replaced{the}{our} \replaced{proposed contrastive language-image pre-training (CLIP)-based}{CLIP-based} slice-selection pipeline. 
    Each \replaced{report}{finding} sentence is encoded by the text \replaced{encoder,}{encoder} and \replaced{each}{every} CT slice by the image encoder. 
    Cosine similarities are computed between the sentence embedding and all slice embeddings, and the slice with the highest score is chosen as the key slice for that finding. 
}
  \label{fig:clip_overview}
\end{figure}


% 2. 本研究で提案するSlice Selectorの概要
\replaced{This}{In} \replaced{study}{this} \replaced{aims}{study, we aim} to \replaced{demonstrate}{clarify} the utility of a slice-selection framework that aligns report sentences with CT slices and to \replaced{investigate}{examine} whether fine-tuning with lesion- and organ-aware supervision can further enhance performance.
\replaced{Contrastive language-image pre-training (CLIP)}{CLIP} jointly embeds images and text into a \replaced{shared}{common} latent space and computes cosine similarity between these embeddings to \replaced{enable}{perform} zero-shot classiﬁcation and text-image retrieval \cite{radford_learning_2021}. 
We \replaced{hypothesized}{hypothesize} that \replaced{the}{this} same framework could be \replaced{adapted}{repurposed} to align individual report sentences with their corresponding CT slices. 
Each finding sentence \replaced{was}{is} \replaced{encoded}{embedded} by the text encoder, and every axial slice in the \added{CT }volume \replaced{was}{is} \replaced{encoded}{embedded} by the image encoder; the slice with the highest cosine similarity \replaced{was}{is} \replaced{selected}{chosen} as the key image for that sentence (Fig. \ref{fig:clip_overview}). 
\replaced{This}{We evaluate this} pipeline \replaced{was evaluated using}{with} the original CLIP (ViT-B/16) and two medical-domain variants, PubMedCLIP and BiomedCLIP \cite{radford_learning_2021,eslami_pubmedclip_2023,zhang_biomedclip_2025}. 
In addition, we curated a compact training set \deleted{drawn }from a limited number of clinical CT studies and fine-tuned each model \replaced{using}{on} this dataset. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Related Works}
\subsection{MedVQA Dataset Construction}
VQA-RAD is a foundational MedVQA benchmark for radiological \replaced{imaging.}{images.} 
Expert radiologists manually authored clinical questions and provided\added{ the} corresponding ground-truth answers, ensuring high annotation quality but resulting in a relatively small dataset \cite{lau_dataset_2018}. 
SLAKE builds on this by adding spatial \replaced{annotations, such}{annotations—such} as pixel-level masks and bounding \replaced{boxes, and}{boxes—and} by including knowledge-base-driven questions to probe deeper clinical reasoning \cite{liu_slake_2021}.
\replaced{The effective}{Effective} training of VLMs in \deleted{the }highly specialized medical \replaced{domains}{domain} requires much larger and more diverse datasets. 
PMC-VQA constructs a \replaced{227-K}{227K} example VQA dataset, predominantly composed of radiological images, using literature extracted from PubMed Central \cite{zhang_pmc-vqa_2024}.
This dataset was built by mining images and their associated captions from \deleted{the }articles and employing ChatGPT to generate the corresponding VQA pairs.
LLava-Med adopts a similar \replaced{strategy:}{strategy.} \replaced{it}{It} first trains\deleted{ on} 600K image-caption pairs mined from PubMed Central \replaced{articles and}{articles. 
It} then \replaced{fine-tunes}{refines} the model with 60K GPT-4-generated instruction-tuning examples \cite{li_llava-med_2023}.
However, none of these datasets provide explicit links between report sentences and specific CT slices, which is the key challenge \replaced{this}{our} study addresses.

% Extraction of slices from Volumetric Dataとか？
\subsection{Slice Selection}
\replaced{
  Vote-MI was proposed as a weakly supervised method for identifying representative key slices from CT volumes \cite{wang_enhancing_2024}. Although effective in highlighting diagnostically relevant slices, Vote-MI is designed for slice selection alone and does not provide a mechanism for sentence-level text–image retrieval. This limits its applicability to tasks such as MedVQA dataset construction, where fine-grained alignment between report sentences and CT slices is required. In contrast, our method explicitly addresses this gap by enabling text-driven slice retrieval and further improving alignment performance through lesion- and organ-aware fine-tuning.
}{
  Vote-MI proposed a method for extracting the most representative slices from 3D MRI volumes of brain tumor cases \cite{wang_enhancing_2024}. 
  The method first employs a variational autoencoder to extract latent features from each slice and then applies clustering to select the key slice for each case. 
  Training the VLMs on the resulting dataset yielded effective learning, although the performance remained below that achieved using expert-curated datasets. 
  Because Vote-MI does not incorporate textual information during slice selection, its applicability is limited to brain tumor imaging and cannot be readily extended to other anatomical regions or multiple disease types.
}
Although no prior \replaced{studies}{study} \replaced{have}{has} applied CLIP to \replaced{slice selection,}{slice-selection,} related methods for extracting \replaced{keyframes}{key frames} from sequential medical data have been employed \replaced{for}{in} zero-shot surgical phase recognition \cite{yuan_hecvl_2025}.


% CLIPの説明
\subsection{Multi Modal Learning}
With the advent of CLIP, contrastive learning has become \replaced{the}{a} predominant paradigm \replaced{in}{for} multimodal representation learning \cite{radford_learning_2021}. 
CLIP projects textual and visual inputs into a shared embedding space by encoding each modality separately—text via a transformer-based text encoder and images via a vision \replaced{transformer—and then}{transformer—then} aligns them through cosine similarity. 
This cross-modal alignment capability has \replaced{led}{prompted} many researchers to employ \deleted{the }CLIP image \replaced{encoders}{encoder} as the visual backbone in VLMs. 
\replaced{Subsequent}{Later} \replaced{studies}{works} \replaced{extended}{have built on} this foundation to better support downstream tasks. For example, GLIP and RegionCLIP introduce region-level contrastive objectives that \replaced{improve}{enhance} object detection and semantic segmentation performance \cite{li_grounded_2022,zhong_regionclip_2022}.
SigLIP \replaced{replaces}{further innovates by replacing} the standard contrastive loss with a \replaced{sigmoid cross-entropy}{sigmoid-cross-entropy} formulation, substantially reducing\added{ the} text-encoder context length while maintaining high-efficiency multimodal learning \cite{zhai_sigmoid_2023}. 
CLIP-Adapter enhances few-shot transfer by inserting lightweight adapter modules into a frozen CLIP backbone, thereby boosting performance on downstream tasks without extensive retraining \cite{gao_clip-adapter_2024}. 
In the medical domain, PubMedCLIP fine-tunes CLIP \replaced{using}{on} ROCO, a radiology-focused corpus derived from PubMed articles \cite{eslami_pubmedclip_2023,pelka_radiology_2018}. 
BiomedCLIP expands this approach by assembling a PMC-15M dataset from PubMed Central publications and \replaced{applying}{using} it \replaced{to}{for} further \added{fine-tune }CLIP\deleted{ fine-tuning} \cite{zhang_biomedclip_2025}. 
These biomedical variants \replaced{have achieved}{achieve} SOTA performance \replaced{in}{on} cross-modal retrieval, zero-shot image classification, and MedVQA tasks, \added{thereby }establishing themselves as foundation models for a \replaced{broad}{wide} range of downstream \replaced{medical AI}{medical-AI} applications \cite{koleilat_medclip-sam_2024,zhao_foundation_2024,polis_exploring_2025}.


In summary, while previous \replaced{research}{work} has advanced MedVQA dataset \replaced{construction,}{creation,} slice retrieval, and multimodal learning, none has proposed an automated framework that aligns report sentences with CT slices and enhances performance through lesion- and organ-aware fine-tuning. This \replaced{constitutes}{forms} the\deleted{ core} novelty of \replaced{the}{our} \replaced{present study.}{work.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Materials and Methods}

\subsection{Dataset}\label{sec:dataset}
% データセット概要
\replaced{This study was approved by Jichi Medical University Hospital Bioethics Committee for Clinical Research.}{This study was approved by the ethics review board of our institution.} 
We assembled our dataset from 137 consecutive \added{patients with }gastrointestinal cancer\deleted{ patients} who underwent their ﬁrst CT examination at our institution between 2021 and 2023. 
\replaced{A}{First,} \replaced{board-certified}{a} radiologist (Radiologist 1) with more than ten years of experience \replaced{reviewed}{read} each report and selected the appropriate CT series (e.g., \deleted{the }optimal contrast phase) for each\deleted{ finding} sentence describing an abnormality. 
The radiologist then \replaced{identified}{chose} the\deleted{ single} CT slice that best matched \replaced{the}{that} sentence and \replaced{annotated the lesion with}{drew} a bounding \replaced{box. 
The}{box} \replaced{dataset}{around} \replaced{was}{the lesion. 
We} divided\deleted{ the dataset} into 92 \replaced{training,}{training studies,} 23 \replaced{validation,}{validation studies,} and 22 test studies (Fig. \ref{fig:dataset_detail}). 
\replaced{For}{In} the test set, the radiologist\deleted{ also} annotated the range of slices corresponding to each sentence to \replaced{enable}{allow} more detailed evaluation. 
This yielded 625 sentence-slice pairs for training, 152 for validation, and 120 for testing. 
The original reports were written in Japanese; after confirming that they contained no protected health information, \replaced{all sentences were}{we} translated\deleted{ all finding sentences} into English using \texttt{GPT-4o-mini} \cite{openai_gpt-4_2024}. 


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./figures/dataset_detail_1.png}
  \caption{
    \added{New figure added in revision.} Overview of dataset \replaced{composition}{details} and evaluation measures.
  }
  \label{fig:dataset_detail}
\end{figure}


% totalsegmentatorによるデータ拡張手法
To improve performance on negative findings, we augmented the dataset with synthetic \replaced{finding–image}{finding-image} pairs \replaced{containing}{that contain} no \replaced{abnormalities.}{abnormal findings.} 
We first applied\added{ the} TotalSegmentator to each CT series to extract organ masks and determine which of the 15 major thoracoabdominal organs were present \replaced{in}{for} each slice \cite{wasserthal_totalsegmentator_2023}. 
Using these organ labels, we generated pseudo-findings in two ways: \replaced{(1)}{by} inserting organ names into a fixed template and \replaced{(2)}{by} prompting \texttt{GPT-4o-mini} to produce \replaced{natural language}{natural-language} descriptions based on the same templates. 
Further details of \replaced{the}{our} \replaced{rule-}{rule-based} and LLM-based prompt designs are provided in Appendix \ref{app:rule_based_text_prompt}.


% 最終的な学習データセット
For lesion-positive examples, we expanded the slice range based on lesion \replaced{size.}{size:} \replaced{Bounding}{bounding} boxes larger than 2,000 pixels were padded by one slice above and below, \replaced{whereas}{while} boxes exceeding 4,000 and 6,000 pixels \replaced{were}{received} \replaced{expanded by two}{two-} and \replaced{three}{three-slice} \replaced{slices,}{expansions,} respectively. 
This procedure yielded a training set composed of 12,743 CT slices, including 1,009 lesion-positive \replaced{image}{finding-image} pairs and 140,761 \replaced{normal anatomy}{normal-anatomy} pairs generated from organ labels.
As part of image preprocessing, we applied soft-tissue windowing to the CT values to enhance contrast in \added{the }relevant intensity ranges. 
Because \replaced{the vision transformer (ViT)}{ViT} backbone requires 224 × 224 inputs, we first cropped 32 pixels from each edge of the original 512 × 512 slices to obtain 448 × 448 images and then downsampled them to 224 × \replaced{224 pixels.}{224.} 
During training, we applied data \replaced{augmentation,}{augmentations} such as horizontal and vertical flips, translations, scaling, rotations, elastic distortions, and \replaced{cutouts,}{cutout} to improve model robustness \cite{devries_improved_2017}.


\subsection{Slice-Selection Algorithm}  

  To \replaced{implement}{operationalize} \replaced{this study’s}{our} approach, we \replaced{defined}{define} a slice-selection algorithm that links each \replaced{report}{finding} sentence to \replaced{the}{its} most relevant CT slice. 
  As illustrated in Fig. \ref{fig:clip_overview}, the text encoder embeds the finding sentence, while the image encoder embeds each axial slice of the CT volume. 
  Cosine similarities are then computed between the sentence embedding and all slice embeddings. 
  The slice with the highest similarity score is designated as the key slice corresponding to the \deleted{given }finding. 
  This procedure, summarized in Alg. \ref{alg:key_slice_selection}, constitutes the inference pipeline of \replaced{this research}{our} method and serves as the foundation for both evaluation and fine-tuning.



\begin{algorithm}
  \caption{CLIP-based Key-Slice Selection}
  \label{alg:key_slice_selection}
  \begin{algorithmic}[1]
    \REQUIRE finding sentence $f$, CT volume $V = {s_1, \dots, s_N}$, text encoder $E_{\mathrm{text}}$, image encoder $E_{\mathrm{img}}$
    \ENSURE key slice $s_k$
    \STATE $e_{\mathrm{text}} \leftarrow E_{\mathrm{text}}(f)$ 
    \FOR{$i \leftarrow 1$ to $N$}
      \STATE $e_{\mathrm{img}} \leftarrow E_{\mathrm{img}}(s_i)$ 
      \STATE $\mathrm{sim}_i \leftarrow \cos\bigl(e_{\mathrm{text}}, e_{\mathrm{img}}\bigr)$ 
    \ENDFOR
    \STATE $k \leftarrow \arg\max_i \mathrm{sim}_i$ 
    \RETURN $s_k$
  \end{algorithmic}
\end{algorithm}


% 学習手法
\subsection{Models and Training Details}
We trained CLIP to learn \replaced{the correspondence}{correspondences} between \replaced{report}{finding} sentences and CT slices. 
For the image encoder, we \replaced{used}{employed} \texttt{ViT-B/16} and fine-tuned the \replaced{pretrained}{pre-trained} model \replaced{with}{using weights provided by OpenAI. 
We applied} the\added{ official OpenAI weights. 
The} same procedure\added{ was applied} to PubMedCLIP and BiomedCLIP. We summarize the image encoders and text encoders used in each method in Table \ref{tab:model_summary}.
For all text encoders, the context length was \replaced{fixed}{set} \replaced{at}{to} 77 tokens, and sentences exceeding this length were truncated.
All training \replaced{was}{took} \replaced{conducted}{place} on a single NVIDIA RTX 6000 Ada GPU with a batch size of \replaced{64}{64,} using the AdamW optimizer \cite{loshchilov_fixing_2017}. 
\replaced{The}{We decayed the} learning rate\added{ was scheduled} from 5e-5 to 1e-6 over 20 epochs using cosine annealing. 
\replaced{The}{Finally, we selected the} checkpoint with the lowest validation loss \added{was selected }for \replaced{evaluation.}{our evaluations.}
The experiments were run on Ubuntu 22.04.5 LTS with Python~3.11.9 (conda-forge), using \texttt{numpy}~2.1.1, \texttt{torch}~2.4.1+cu118, and \texttt{TotalSegmentator}~2.10.0.

\begin{table}[ht]
  \centering
  \caption{\added{New table added in revision.} Summary of image and text encoders used in each method.}
  \label{tab:model_summary}
  \begin{tabular}{lcc}
    \toprule
    Method                  & Image Encoder      & Text Encoder       \\
    \midrule
    CLIP                    & ViT-B/16          & Transformer        \\
    PubMedCLIP              & ViT-B/32          & Transformer        \\
    BiomedCLIP              & ViT-B/16          & PubMedBERT         \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

% セクション全体の説明
We evaluated the \replaced{slice-selection}{slice selection} performance of six models: \replaced{pretrained}{the pre-trained} CLIP, PubMedCLIP, and BiomedCLIP, along with their fine-tuned counterparts (FT). 

  Before reporting task-specific retrieval metrics, we first verified whether each model\deleted{ had} learned meaningful alignments during training. To this end, we measured the mean absolute error (MAE) between the predicted and ground-truth slice indices \replaced{using}{under} two strategies: hard prediction (selecting the top-1 slice) and soft prediction (using a local moving average).

In Section \ref{sec:lesion_aware}, we assess \added{the }retrieval accuracy \replaced{of}{on} sentences describing\deleted{ actual} abnormal findings from diagnostic reports. 
In addition, we \replaced{report on}{conducted} a \replaced{radiologist-led}{radiologist} evaluation of \replaced{slice}{the} \replaced{selection}{slice-selection} results to examine clinical applicability.
Section \ref{sec:organ_aware} \replaced{presents}{reports} the alignment fidelity between organ-related text and its corresponding CT slices. 
Finally, Section \ref{sec:visualization} \replaced{provides}{presents} visualizations of the \replaced{slice-selection}{slice selection} results \replaced{for}{on} key series, \replaced{highlighting}{illustrating} both quantitative performance and qualitative interpretability.

\subsection{Training Verification via Slice-Level MAE}

  To validate the training process, we measured the mean absolute error (MAE) between the predicted and ground-truth slice indices across all models using the image-sentence pairs from the validation set. 
  We compared two strategies: (i) hard prediction, which directly selects the slice with the highest similarity score, and (ii) soft prediction, which applies a moving average over $\pm\added{ 2}$\deleted{2} neighboring slices to \replaced{leverage}{exploit} the sequential continuity of CT volumes. 
  As shown in Fig. \ref{fig:mae}, fine-tuned models consistently achieved lower MAE than their pre-trained counterparts, confirming that meaningful learning occurred. 
  Moreover, the soft prediction strategy yielded \replaced{fewer}{lower} errors than \added{the }hard \replaced{prediction strategy,}{prediction,} motivating \replaced{its}{our} \replaced{adoption}{decision to adopt the soft prediction setting} for all subsequent evaluations. For example, in BiomedCLIP, the MAE improved from $10.40 \pm 13.58$ with hard prediction to $9.21 \pm 11.09$ with soft prediction, corresponding to an average gain of approximately one slice in localization accuracy.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./figures/mae.png}
  \caption{
    \added{New figure added in revision.}
    Comparison of mean absolute error (MAE) between predicted and ground-truth slice indices for each method. 
    \replaced{Results are shown for both}{Both} hard prediction (top-1 slice) and soft prediction (moving average over $\pm$2 \replaced{slices).}{slices) are shown.}
  }
  \label{fig:mae}
\end{figure}


\subsection{Lesion Awareness}\label{sec:lesion_aware}
\replaced{The models are}{We} evaluated \replaced{using}{the models on} a test set of 22 studies containing 120 \deleted{finding }sentences with manually annotated ground-truth slice ranges. 
At \replaced{inference,}{inference time,} each sentence \replaced{is}{was} encoded by the CLIP text encoder, every slice in the corresponding CT volume \replaced{is}{was} processed by the image encoder, cosine similarities \replaced{are}{were} computed, and the slice with the highest score \replaced{is}{was} \replaced{selected}{taken} as the \replaced{model’s}{model-selected} key slice (Fig. \ref{fig:clip_overview}, Alg. \ref{alg:key_slice_selection}). 
To assess \replaced{interobserver}{inter-observer} variability, Radiologist 2—independent of the ground-truth annotator—also selected the single best-matching slice for each sentence.
Table \ref{tab:slice_extraction_accuracy} shows \deleted{both }the proportion of Top-1 predictions falling within the annotated ranges and the proportion of cases \replaced{in which}{where} any of the Top-5 most similar slices \replaced{overlapped}{overlap} with those ranges. 
CLIP (FT) \replaced{improves the}{improved} Top-1 accuracy by 20 percentage points over\deleted{ the} CLIP, demonstrating effective adaptation to the medical domain.
\deleted{Both }PubMedCLIP and BiomedCLIP began with higher baseline \replaced{accuracies,}{accuracies} \replaced{which are}{and} further improved \replaced{by}{after} fine-tuning. 
\replaced{BiomedCLIP (FT), in}{In} particular, \replaced{achieves}{BiomedCLIP (FT) achieved} the best \replaced{performance,}{results,} with a Top-1 accuracy of 51.72\% and \deleted{a }Top-5 accuracy of 64.37\%. 
Considering that \replaced{Radiologist}{a} \replaced{2}{second radiologist} achieved a Top-1 accuracy of 78.16\%, the \replaced{model accuracy of}{model’s} 51.72\% \replaced{represents}{can be viewed as} a relatively strong result.



\begin{table}[ht]
  \centering
  \caption{Comparison of slice-selection accuracy for each method. 
  Reported values are Top-1 and Top-5 accuracy[\%]. 
  For each metric, the highest score among the automated methods (excluding the \replaced{Radiologist 2)}{Radiologist2)} is highlighted in red.}
  \label{tab:slice_extraction_accuracy}
  \begin{tabular}{lcc}
    \toprule
    Method                  & Acc.@1,↑      & Acc.@5,↑       \\
    \midrule
    Radiologist2            & 78.16       & -            \\
    CLIP                    & 19.54       & 44.83        \\
    CLIP(FT)                & 40.23       & 49.43        \\
    PubMedCLIP              & 29.89       & 54.02        \\
    PubMedCLIP(FT)          & 42.53       & 59.77        \\
    BiomedCLIP              & 44.83       & 60.92        \\
    BiomedCLIP(FT)          & \textcolor{red}{51.72} & \textcolor{red}{64.37}  \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[ht]
  \centering
  \caption{Radiologist acceptance rates[\%] of slice selector predictions for each model.}
  \label{tab:slice_selector_adoption}
  \begin{tabular}{lc}
    \toprule
    Method                  & Acceptance Rate,↑  \\
    \midrule
    CLIP                    & 24.14         \\
    CLIP(FT)                & 40.23         \\
    PubMedCLIP              & 37.93         \\
    PubMedCLIP(FT)          & 50.57         \\
    BiomedCLIP              & 50.57         \\
    BiomedCLIP(FT)          & 56.32         \\
    \bottomrule
  \end{tabular}
\end{table}


% \section{Radiologist Evaluation of Slice Selection Results}\label{app:doctor_evaluation}
As noted above, \deleted{we recognized that }strict ground-truth ranges do not always align with \replaced{radiologists’}{radiologist} subjective assessments.
\replaced{To}{Therefore, to} evaluate whether the slices proposed by each method would be considered acceptable in clinical practice, we conducted a qualitative assessment in which a radiologist evaluated whether to adopt each suggested slice \deleted{outright }for a given\deleted{ finding} sentence. 
We developed a simple web application that displayed each finding sentence alongside the slices proposed by each slice selector. 
For each case, \added{the }suggestions from all models were presented in\added{ a} random order on the same screen, and the radiologist was blinded to the source model. 
Table \ref{tab:slice_selector_adoption} reports the adoption \replaced{rates}{rate} for each model’s recommendations. 
\replaced{Although}{While} these adoption rates generally correspond to the quantitative results \replaced{presented}{shown} in Table \ref{tab:slice_extraction_accuracy} (Top-1 accuracy), they are uniformly slightly higher. 
These higher adoption rates \replaced{occur}{arise} when lesions \replaced{deemed acceptable by}{that} the radiologist\deleted{ still finds acceptable} fall outside the strict ground-truth range—for example, calcifications \replaced{extending}{that extend} across several slices or \deleted{cases where the }annotation \replaced{covering}{covers} only one of multiple hepatic cysts.


\subsection{Organ Awareness}\label{sec:organ_aware}
Recognizing normal anatomy is as important as detecting abnormalities, so we deﬁne organ-aware slice selection as a complementary evaluation. 
Using TotalSegmentator, we generated binary \replaced{organ-presence}{organ presence} labels for each CT slice based on\added{ the} 15 major thoracoabdominal organs \cite{wasserthal_totalsegmentator_2023}. 
For each model, we encoded the organ prompt and \replaced{each}{every} CT slice, computed their cosine similarities, and then determined organ-specific thresholds \replaced{using}{(via} the Youden index on the validation \replaced{set,}{set)} \replaced{which}{that} served as \replaced{cutoffs}{the cut-off} for \replaced{determining}{deciding whether a given} organ \replaced{presence}{was present} in each \deleted{slice on }test \replaced{slice}{set} \cite{youden_index_1950}. 
To \replaced{assess}{explore} the impact of text granularity, we compared two input types: single-word organ names (e.g., “Heart,” “Liver”) and full template sentences \replaced{(e.g.,}{such as} “This CT image includes the \replaced{heart”).
}{heart.” 
}These experiments \replaced{test}{inform} whether \replaced{word-}{to use word-level} or sentence-level prompts \added{are more effective }when querying \replaced{a}{the} text encoder. 


Table \ref{tab:organ_extraction_results} summarizes the organ-aware evaluation, reporting both \added{the }accuracy and F1-score for each model under \replaced{word-}{Word} and \replaced{sentence-prompt}{Sentence prompt} conditions. 
BiomedCLIP (FT) achieved the highest performance in both settings, with an F1-score of 0.85 for Word and Sentence inputs. 
PubMedCLIP \replaced{showed}{exhibited} similar accuracy and F1-scores across the two prompt types, \replaced{while}{whereas} BiomedCLIP tended to perform better with \replaced{sentence}{Sentence} prompts. 
Fine-tuning yielded \replaced{comparable}{roughly equivalent} gains for Word and Sentence inputs across all models, indicating that our approach \replaced{generalized}{generalizes} well to both single-token organ names and \replaced{fully}{full} descriptive sentences. 
On average, fine-tuning improved\added{ the} accuracy and F1-score by more than 10\% for \replaced{each}{every} model. 
These substantial improvements\deleted{ not only} demonstrate enhanced normal-anatomy recognition accuracy\added{ not only} for the original CLIP model but also for the medical-domain pre-trained variants \replaced{(PubMedCLIP}{PubMedCLIP} and \replaced{BiomedCLIP).}{BiomedCLIP.} 
They\added{ further} suggest that fine-tuning \replaced{improves}{can improve} slice extraction accuracy \replaced{for}{from} non-abnormal \replaced{findings,}{findings} \replaced{thereby}{and,} \replaced{benefiting}{consequently, benefit} downstream VQA generation.
F1-scores for each organ are provided in Appendix \ref{app:organ_aware_details}.


\begin{table}[ht]
  \centering
  \caption{Comparison of organ extraction performance. Reported metrics are accuracy (Acc.) and F1-score (F1). 
  “Word” indicates prompts using single organ names, and “Sentence” indicates prompts using full finding sentences. 
  The highest score in each column is highlighted in red.}
  \label{tab:organ_extraction_results}
  \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Word}               & \multicolumn{2}{c}{Sentence}            \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
                           & Acc.,↑ & F1,↑         & Acc.,↑   & F1,↑          \\ 
    \midrule
    CLIP                    & $0.715\pm0.105$ & $0.514\pm0.182$ & $0.725\pm0.071$ & $0.520\pm0.192$  \\
    CLIP(FT)                & $0.927\pm0.038$ & $0.806\pm0.153$ & $0.934\pm0.035$ & $0.824\pm0.129$  \\
    PubMedCLIP              & $0.881\pm0.046$ & $0.722\pm0.164$ & $0.882\pm0.042$ & $0.726\pm0.157$  \\
    PubMedCLIP(FT)          & $0.943\pm0.030$ & $0.839\pm0.123$ & $0.944\pm0.031$ & $0.843\pm0.121$  \\
    BiomedCLIP              & $0.832\pm0.133$ & $0.673\pm0.230$ & $0.884\pm0.048$ & $0.730\pm0.126$  \\
    BiomedCLIP(FT)          & \textcolor{red}{$0.947\pm0.033$} & \textcolor{red}{$0.853\pm0.118$} & \textcolor{red}{$0.948\pm0.032$} & \textcolor{red}{$0.854\pm0.118$}  \\
    \bottomrule
  \end{tabular}
\end{table}



Additionally, as an external dataset benchmark, we evaluated organ awareness using the publicly available CT-RATE dataset, which consists of chest CT volumes with organ labels generated by TotalSegmentator \cite{hamamci_ct2rep_2024}. 
We \replaced{evaluated}{conducted evaluation on} 1,304 cases from the validation split, \replaced{selecting}{using} the earliest series (denoted as “\_a\_1” in the dataset) when multiple CT series were available per patient. 
The results are summarized in Table \ref{tab:organ_extraction_results_ctrate}. 
Overall performance was lower than \replaced{that}{those} reported in Table \ref{tab:organ_extraction_results}, likely due to domain shifts related to imaging devices and acquisition parameters. 
For \replaced{example,}{instance,} BiomedCLIP(FT) achieved an accuracy of 0.926 and an F1-score of 0.809 on CT-RATE, compared \replaced{with}{to} 0.948 and 0.854, respectively, \replaced{on}{in} \replaced{the}{our} in-house dataset. 
Despite \replaced{this}{the overall} decrease, BiomedCLIP(FT) consistently yielded the highest accuracy \replaced{across}{among} all models, confirming its robustness \replaced{to}{across} \replaced{dataset variation.}{datasets.}



\begin{table}[ht]
  \centering
  \caption{\added{New table added in revision.} Comparison of organ extraction performance on the external CT-RATE dataset. 
  Reported metrics are accuracy (Acc.) and F1-score (F1). 
  “Word” indicates prompts using single organ names, and “Sentence” indicates prompts using full finding sentences. 
  The highest score in each column is highlighted in red.}
  \label{tab:organ_extraction_results_ctrate}
  \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Word}               & \multicolumn{2}{c}{Sentence}            \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
                          & Acc.,↑ & F1,↑         & Acc.,↑   & F1,↑           \\
    \midrule
    CLIP                    & $0.726\pm0.059$ & $0.554\pm0.223$ & $0.724\pm0.065$ & $0.556\pm0.226$  \\
    CLIP(FT)                & $0.902\pm0.050$ & $0.770\pm0.251$ & $0.902\pm0.048$ & $0.770\pm0.250$  \\
    PubMedCLIP              & $0.842\pm0.099$ & $0.680\pm0.237$ & $0.861\pm0.095$ & $0.707\pm0.226$  \\
    PubMedCLIP(FT)          & $0.911\pm0.072$ & $0.796\pm0.252$ & $0.910\pm0.073$ & $0.795\pm0.252$  \\
    BiomedCLIP              & $0.749\pm0.171$ & $0.632\pm0.271$ & $0.755\pm0.125$ & $0.604\pm0.226$  \\
    BiomedCLIP(FT)          & \textcolor{red}{$0.926\pm0.059$} & \textcolor{red}{$0.809\pm0.257$} & \textcolor{red}{$0.926\pm0.059$} & \textcolor{red}{$0.809\pm0.257$}  \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Visualization}\label{sec:visualization}
Fig. \ref{fig:lesion_aware_results} shows an example of \added{a }lesion-aware slice-selection result on \replaced{the}{our} test cohort. 
For\deleted{ a case of} uterine fibroids, PubMedCLIP, BiomedCLIP, and \replaced{BiomedCLIP(FT)}{BiomedCLIP (FT) all} correctly \replaced{identified}{identify the} key CT \replaced{slices}{slice} containing the lesion.
Fig. \ref{fig:organ_aware_results} shows similarity profiles between organ-related text and CT slices for a selected series. 
The original CLIP model \replaced{exhibited}{exhibits} numerous false positives and inconsistent \replaced{alignments,}{alignment,} whereas CLIP (FT) \replaced{produced}{produces} a much cleaner correspondence and improved localization. 
Although PubMedCLIP and BiomedCLIP also \replaced{showed}{display some} false positives and \deleted{false }negatives, both models \replaced{achieved}{achieve} high accuracy in recognizing the target organ after fine-tuning. 
Additional qualitative examples are \replaced{presented}{provided} in Appendix \ref{app:visualization}.


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/figure2_4.png}
  \caption{"Multiple masses that may represent fibroids are observed in the myometrium and beneath the endometrium."
  The Ground Truth column shows the reference CT slice with the radiologist-annotated lesion bounding box.
  The other columns display each model’s key-slice prediction.
  The white double circle symbol indicates predictions that match the ground-truth slice range.
  }
  \label{fig:lesion_aware_results}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/figure3_4.png}
  \caption{Similarity profile between the prompt “Heart” and each slice. 
  The green line denotes the ground-truth slice range. 
  Similarity profiles increase from left to right, and the dashed line indicates the \added{decision }threshold. 
  }
  \label{fig:organ_aware_results}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

In this study, we introduced a CLIP-based key-slice selector that aligns free-text radiology findings with \deleted{their }corresponding CT slices. 
\replaced{The approach was}{We} evaluated\deleted{ our approach} along two complementary axes: lesion\deleted{ awareness} and organ awareness. 
Despite training on a relatively small dataset, \added{the }fine-tuned models achieved substantial gains in lesion-aware slice \replaced{retrieval, improving}{retrieval—improving} accuracy by \replaced{7–20}{7 to 20} percentage points over the pre-trained baselines. 
The best performer, BiomedCLIP (FT), \replaced{achieved}{reached} 51\% Top-1 accuracy for lesion localization. 
In the organ-aware task, all fine-tuned models \replaced{achieved improvements of}{realized} over 10 \%\deleted{ improvements} in both accuracy and F1-score. 
Notably, \added{the }performance remained strong\added{ regardless of} whether organ prompts were provided as single words or full sentences, demonstrating the flexibility of \replaced{the}{our} text-encoding strategy. 

  \replaced{To}{Moreover,} \replaced{further}{to} examine\added{ the} robustness under external conditions, we conducted an additional organ-awareness evaluation on the publicly available CT-RATE dataset. 
  \replaced{Although}{While} all methods exhibited some degradation in performance \replaced{owing}{due} to domain shifts in imaging devices and acquisition protocols, the decrease was relatively modest, and \replaced{BiomedCLIP (FT)}{BiomedCLIP(FT)} consistently achieved the highest accuracy. 
  These findings suggest that \replaced{this study’s}{our} approach \replaced{is}{maintains} \replaced{robust}{robustness} across datasets and can \replaced{be generalized}{generalize} beyond \replaced{the}{our} in-house cohort.



  \replaced{Regarding}{In terms of} system requirements, all experiments were conducted on a single NVIDIA RTX 6000 Ada GPU with 48 GB \added{of }memory, which was sufficient to train\deleted{ the} models with a batch size of 64. 
  Each training run \replaced{with}{of} 20 epochs finished within one hour. 
  These results indicate that \replaced{the proposed}{our} method can be reproduced on a high-end single-GPU workstation without requiring distributed resources.



  Beyond \replaced{the}{our} \replaced{dataset}{own} \replaced{used in this study,}{dataset,} the proposed slice-selection framework offers practical benefits when applied to diverse imaging archives. 
  First, \deleted{by }automatically aligning report sentences with key \replaced{slices}{slices, it} can substantially reduce the cost of \replaced{two-dimensional (2D)}{2D} slice-level annotation, which is often \replaced{a}{the} major bottleneck in dataset curation. 
  Second, it enables the construction of image-text pairs directly from unlabeled CT volumes and routine reports, providing a low-cost pathway \replaced{for}{to} \replaced{expanding}{expand} training resources for medical VLMs. 
  These improvements highlight the broader utility of \replaced{the proposed}{our} method across datasets and institutions, extending its impact beyond the present feasibility study.


\replaced{The}{Our} method paves the way for fully automated construction of MedVQA datasets from routine clinical archives, directly addressing \added{the }biases and scale limitations inherent in \replaced{bibliographic}{bibliographic-based} collections. 
Furthermore, the \replaced{radiologist’s}{radiologist} evaluation of \replaced{the slice selection}{slice-selection} results (Table \ref{tab:slice_selector_adoption}) and\deleted{ the} visualization of similarity profiles (Fig. \ref{fig:organ_aware_results}) suggest that, in addition to serving as a VQA dataset generator, integrating the slice selector into clinical workflows as a recommendation tool could help reduce radiologists’ workload and enhance diagnostic accuracy. 


\textbf{Limitations}
First, \replaced{as}{because} this \replaced{was}{is} a feasibility study, the training and test cohorts \replaced{were}{are} small and drawn from a single institution. Expanding the dataset size could uncover additional gains in retrieval accuracy and reduce the risk of overﬁtting to \replaced{the}{our} relatively small evaluation cohorts. 
Second, \replaced{the}{our} evaluation reports only overall lesion- and organ-aware metrics; it does not stratify \added{the }results by lesion size, anatomical region, contrast phase, or disease category. \replaced{Finer-grained}{A finer-grained} analysis is essential \replaced{to}{for} \replaced{uncover}{uncovering} failure modes in clinically relevant subgroups. 
Third, the Youden-index thresholds used to \replaced{determine}{decide} organ presence were chosen \replaced{from}{on} the validation set and\deleted{ then} fixed for testing. Although this mirrors a real-world deployment scenario, alternative calibration strategies \replaced{may}{might} yield better threshold stability \replaced{for}{on} unseen data. 


\textbf{Future Work}
In \replaced{future}{the} \replaced{work,}{future,} we will integrate \replaced{a}{the} key-slice selector into a complete MedVQA-generation \deleted{pipeline. 
This }pipeline \replaced{consisting}{consists} of four main \replaced{stages.}{stages:} 
\begin{enumerate}
  \item segmenting diagnostic reports into individual finding units, 
  \item acquiring the corresponding imaging series (including selection of contrast phases),
  \item extracting key slices using \replaced{the}{our} slice selector, and 
  \item generating VQA pairs from each finding sentence through rule-based or LLMs.
\end{enumerate}
While \replaced{this}{our} study \replaced{addresses}{has} \replaced{Stage}{addressed stage} 3, the remaining steps are\deleted{ equally} critical for end-to-end automation.
Accordingly, we plan to develop and evaluate methods for report segmentation and image retrieval, and to implement VQA generation using both deterministic templates and generative LLM prompts. 
As these components mature, we will iteratively expand \replaced{the}{our} clinical dataset and refine the slice selector’s accuracy. 

  We also plan to train VLMs on the resulting automatically constructed MedVQA dataset and compare their performance \replaced{with }{against 
  }models trained on manually curated slice annotations to quantify the impact of automated slice selection on \added{the }downstream VLM capabilities.



  Beyond dataset construction, architectural advances offer \deleted{another }promising \replaced{directions.}{direction.} 
  Recent large-scale VLMs such as Merlin, which align 3D CT volumes with radiology reports through contrastive learning, provide useful design principles \cite{blankemeier_merlin_2024}. 
  Incorporating similar volume-level representation strategies, as well as techniques from the CLIP-Driven Universal \replaced{Model—which}{Model} \replaced{uses}{that use} text embeddings as semantic labels for segmentation and \replaced{detection—could}{detection, 
  could} extend \replaced{the}{our} framework beyond slice retrieval \replaced{to}{toward} \replaced{fine-grained}{finer-grained} localization and more efficient MedVQA dataset construction \cite{10376801}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

This study \replaced{introduced}{presented} a CLIP-based key-slice selector that aligns free-text radiology findings with \replaced{the}{their} corresponding CT images. Fine-tuning on a compact dual-supervised dataset \replaced{substantially}{led} \replaced{improved}{to a notable improvement in} lesion-aware accuracy, with the best-performing model achieving strong results in lesion localization. In the organ-aware task, all fine-tuned models demonstrated \replaced{gains}{improvements} in both accuracy and F1-score, regardless of whether \added{the }prompts were\added{ provided as} single words or full sentences. A qualitative review indicated that the slice selector’s recommendations could reduce radiologists’ workload and enhance diagnostic confidence in routine practice. \replaced{Nevertheless,}{However,} the current evaluation is limited by \replaced{its}{a} single-institution, small-cohort design, \replaced{lack}{the absence} of \replaced{stratification by lesion size}{lesion-size} or \replaced{disease}{disease-specific} \replaced{type,}{analyses,} and \replaced{reliance}{the} \replaced{on}{use of} fixed threshold calibration, all of which limit \replaced{generalizability.}{generalisability.} Future work will expand the dataset, incorporate stratified analyses, and integrate the selector into an end-to-end MedVQA pipeline \replaced{encompassing}{that includes} report segmentation, series retrieval, slice extraction, and automatic question-answer generation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following supporting information can be downloaded at:  \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.}

% Only for journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title. A supporting video article is available at doi: link.}

% Only used for preprtints:
% \supplementary{The following supporting information can be downloaded at the website of this paper posted on \href{https://www.preprints.org/}{Preprints.org}.}

% Only for journal Hardware:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.\vspace{6pt}
%\begin{tabularx}{\textwidth}{lll}
%\toprule
%\textbf{Name} & \textbf{Type} & \textbf{Description} 
%\midrule
%S1 & Python script (.py) & Script of python source code used in XX 
%S2 & Text (.txt) & Script of modelling code used to make Figure X 
%S3 & Text (.txt) & Raw data from experiment X 
%S4 & Video (.mp4) & Video demonstrating the hardware in use 
%... & ... & ... 
%\bottomrule
%\end{tabularx}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, K.Y. and T.K.; methodology, K.Y.; software, K.Y.; validation, K.Y.; formal analysis, K.Y.; investigation, K.Y. and T.K.; resources, T.K.; data curation, K.Y. and T.K.; writing---original draft preparation, K.Y.; writing---review and editing, T.K.; visualization, K.Y.; supervision, T.K.; project administration, K.Y. All authors have read and agreed to the published version of the manuscript.}

\funding{This research received no external funding.}

\institutionalreview{The study was conducted in accordance with the Declaration of Helsinki, and approved by the Jichi Medical University Hospital Bioethics Committee for Clinical Research (protocol code 24-061 and date of approval 25 September 2024).}

\informedconsent{This retrospective study was approved by the Jichi Medical University Hospital Bioethics Committee for Clinical Research, and the requirement for informed consent was waived due to the retrospective nature of the study and the use of anonymized medical imaging data that poses minimal risk to participants.}

\dataavailability{The datasets generated and analyzed during the current study are not publicly available due to privacy and ethical restrictions related to patient medical imaging data. The source code used for the analysis is available from the corresponding author \replaced{upon}{on} reasonable request.}

\conflictsofinterest{The authors declare no conflicts of interest.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional

%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

\abbreviations{Abbreviations}{
The following abbreviations are used in this manuscript:


\noindent 
\begin{tabular}{@{}ll}
CLIP & Contrastive Language-Image Pre-training\\
CT & Computed Tomography\\
VLM & Vision-Language Model\\
MedVQA & Medical Visual Question Answering\\
LLM & Large Language Model\\
SOTA & State-of-the-Art\\
VQA & Visual Question Answering\\
\end{tabular}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixstart
\appendix
\section{Findings Text Generation}\label{app:rule_based_text_prompt}
The following templates were designed to generate non-abnormal finding sentences using organ labels predicted by TotalSegmentator. 
In each template, the placeholder \texttt{{{organ}}} \replaced{was}{is} replaced with the\added{ corresponding} organ label obtained from TotalSegmentator. 
Note that the original templates \replaced{were}{are} written in Japanese and \replaced{later}{then} translated into \replaced{English;}{English,} \replaced{therefore,}{so} their word order may differ\added{ slightly} from natural English syntax.
\subsection{Rule-based}
\begin{verbatim}
  organs = [
      "aorta",
      "colon",
      "duodenum",
      "esophagus",
      "gallbladder",
      "heart",
      "kidney",
      "liver",
      "pancreas",
      "stomach",
      "spleen",
      "thyroid_gland",
      "urinary_bladder",
      "trachea",
      "lung",
  ]
  text_templates = {
    "prefix": [
      "In the image,",
      "In this image,",
      "In this CT \replaced{image,”,}{image,",}
      "",
    ],
    "suffix": [
      "{{organ}} is visible."
      "{{organ }} is included."
      "{{organ}} is contained."
      "{{organ}} etc. are visible."
      "{{organ }} etc. are included."
      "{{organ }} etc. are contained."
    ],
  }
\end{verbatim}
\subsection{LLMs-based}
\replaced{We}{Since we} generated 20\added{ sentence} templates for each \replaced{organ. Below,}{organ,} we present \replaced{representative}{only} \replaced{examples}{those} for the liver and \replaced{pancreas.}{pancreas here.}
\begin{verbatim}
  {
    'liver': [
      'No abnormal findings are observed in the liver, and normal morphology is maintained.',
      \replaced{''No}{'No} abnormal findings are observed in the liver, 
      and morphology falls within normal limits.',
      'No lesions or abnormal findings are noted in the liver.',
      'No abnormal findings are observed in the liver, and normal morphology is maintained.',
      'No abnormal findings are observed in the liver, and it remains within normal limits.',
      'No abnormal masses or tumors can be identified in the liver.',
      'No abnormal findings are observed in the liver, and its structure is normal.',
      'No abnormal findings are observed in the liver, and its normal state is preserved.',
      'No abnormal findings are observed in the liver, 
      and its shape and size are within normal limits.',
      'No abnormal masses or fatty changes are noted in the liver.',
      'No abnormal masses or deformities are observed in the liver, 
      and findings are within normal limits.',
      'No findings indicative of abnormality are observed in the liver.',
      'No deformities or neoplastic lesions are observed in the liver, 
      which maintains normal morphology.',
      'No morphological abnormalities or neoplastic lesions are observed in the liver.',
      'No significant lesions or impairments are detected in the liver.',
      'No particular abnormalities are observed in the liver, 
      which preserves normal morphology.',
      'No abnormalities in shape or echo texture are noted in the liver.',
      'No obvious lesions or abnormal findings are noted in the liver.',
      'No abnormal masses or perfusion changes are observed in the liver.',
      'No abnormal findings are observed in the liver, and its normal structure is preserved.'
    ],
  'pancreas': [
    'No abnormal findings are observed in the pancreas.',
    'No findings suggestive of abnormality are noted in the pancreas.',
    'No significant abnormalities are observed in the pancreas.',
    'No findings indicating abnormality are apparent in the pancreas.',
    'No abnormal findings are observed in the pancreas, and normal morphology is maintained.',
    'No abnormal masses or signs of inflammation are observed in the pancreas.',
    'No masses or inflammatory findings are noted in the pancreas.',
    'No abnormal findings are observed in the pancreas, 
    and its morphology falls within normal limits.',
    'No tumors or inflammatory findings are observed in the pancreas, 
    which retains normal morphology.',
    'No findings suggestive of abnormality are apparent in the pancreas.',
    'No structural abnormalities or lesions are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas.',
    'No tumors or abnormal structural changes are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas, 
    and normal imaging appearance is obtained.',
    'No abnormal findings are observed in the pancreas, 
    and normal morphology is confirmed.',
    'No abnormal masses or inflammation are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas.',
    'No abnormal findings of any kind are observed in the pancreas.',
    'No abnormal findings are observed in the pancreas, and normal morphology is present.',
    'No findings suggestive of lesions are observed in the pancreas.'
    ]
  }
\end{verbatim}


% \section{Radiologist Evaluation of Slice Selection Results}\label{app:doctor_evaluation}
% In addition to supporting automated MedVQA dataset creation, our slice selector can serve as a clinical decision support tool by 
% recommending the most relevant CT slices to radiologists. 
% To assess this utility, we conducted a qualitative evaluation in which a radiologist reviewed the slice suggestions generated by 
% each model for a series of finding sentences. 
% Rather than measuring strict overlap with annotated ground truth, 
% the radiologist indicated whether they would accept the recommended slice as appropriate for the given finding.
% The radiologist who participated in this evaluation was not the same individual who performed the dataset annotations. これは菊地先生だわ。
% We developed a simple web application to display each finding sentence alongside the proposed slices. 
% For each case, the suggestions from all models were presented in random order, and the radiologist was blinded to model identity. 
% Table \ref{tab:slice_selector_adoption} reports the adoption rate for each model’s recommendations, 
% defined as the percentage of suggestions the radiologist deemed acceptable.
% The adoption rates generally mirror the quantitative Top-1 accuracy results shown in Table \ref{tab:slice_extraction_accuracy}, 
% although overall rates tended to be slightly higher. 
% We attribute this to lesions such as calcifications that span broader regions: even when a suggested slice falls outside the strict ground-truth interval, 
% it may still be clinically acceptable to the radiologist.


\clearpage
\begin{landscape}


\section{Detailed Organ-Aware Results}\label{app:organ_aware_details}


\begin{table}[ht]
  \centering
  \caption{F1 scores for each organ across slice selection methods. 
  “Word” denotes prompts using only the organ name, 
  and “Sentence” denotes prompts using full finding sentences.}
  \label{tab:organ_f1_comparison}
  \footnotesize
  \begin{tabular}{l*{6}{cc}}
    \toprule
    Organ
      & \multicolumn{2}{c}{CLIP}
      & \multicolumn{2}{c}{CLIP(FT)}
      & \multicolumn{2}{c}{PubMedCLIP}
      & \multicolumn{2}{c}{PubMedCLIP(FT)}
      & \multicolumn{2}{c}{BiomedCLIP}
      & \multicolumn{2}{c}{BiomedCLIP(FT)}  \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}%
    \cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}
      & word & sentence
      & word & sentence
      & word & sentence
      & word & sentence
      & word & sentence
      & word & sentence  \\
    \midrule
    aorta             & 0.679 & 0.660 & 0.960 & 0.960 & 0.852 & 0.836 & 0.966 & 0.967 & 0.858 & 0.754 & 0.943 & 0.948  \\
    colon             & 0.676 & 0.747 & 0.877 & 0.877 & 0.817 & 0.859 & 0.880 & 0.878 & 0.878 & 0.886 & 0.878 & 0.878  \\
    duodenum          & 0.450 & 0.522 & 0.761 & 0.775 & 0.749 & 0.762 & 0.804 & 0.841 & 0.665 & 0.724 & 0.798 & 0.828  \\
    esophagus         & 0.614 & 0.731 & 0.932 & 0.931 & 0.936 & 0.918 & 0.958 & 0.958 & 0.873 & 0.895 & 0.973 & 0.972  \\
    gallbladder       & 0.250 & 0.235 & 0.494 & 0.495 & 0.450 & 0.472 & 0.547 & 0.547 & 0.432 & 0.451 & 0.543 & 0.537  \\
    heart             & 0.554 & 0.643 & 0.933 & 0.941 & 0.842 & 0.832 & 0.945 & 0.943 & 0.664 & 0.841 & 0.956 & 0.955  \\
    kidney            & 0.684 & 0.576 & 0.886 & 0.887 & 0.767 & 0.788 & 0.885 & 0.884 & 0.733 & 0.696 & 0.917 & 0.917  \\
    liver             & 0.600 & 0.570 & 0.900 & 0.901 & 0.791 & 0.832 & 0.928 & 0.921 & 0.821 & 0.750 & 0.914 & 0.912  \\
    pancreas          & 0.588 & 0.518 & 0.801 & 0.807 & 0.731 & 0.716 & 0.836 & 0.841 & 0.730 & 0.755 & 0.838 & 0.824  \\
    stomach           & 0.402 & 0.418 & 0.720 & 0.737 & 0.650 & 0.612 & 0.780 & 0.767 & 0.699 & 0.673 & 0.823 & 0.827  \\
    spleen            & 0.325 & 0.409 & 0.731 & 0.744 & 0.619 & 0.685 & 0.809 & 0.811 & 0.665 & 0.639 & 0.810 & 0.800  \\
    thyroid gland     & 0.363 & 0.227 & 0.744 & 0.744 & 0.552 & 0.576 & 0.648 & 0.647 & 0.219 & 0.643 & 0.727 & 0.728  \\
    urinary bladder   & 0.165 & 0.210 & 0.491 & 0.691 & 0.387 & 0.386 & 0.739 & 0.779 & 0.160 & 0.575 & 0.753 & 0.762  \\
    trachea           & 0.542 & 0.514 & 0.900 & 0.905 & 0.730 & 0.714 & 0.881 & 0.884 & 0.796 & 0.767 & 0.945 & 0.947  \\
    lung              & 0.816 & 0.827 & 0.964 & 0.963 & 0.954 & 0.904 & 0.976 & 0.975 & 0.906 & 0.907 & 0.980 & 0.981  \\
    \bottomrule
  \end{tabular}
\end{table}
\end{landscape}


\section{Visualization}\label{app:visualization}
\subsection{Lesion Awareness}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst6_2.png}
  \caption{Visualization of lesion-aware slice selection for the \replaced{finding: }{finding 
  }"Circumferential wall thickening is observed in the antrum to pylorus of the stomach, which is consistent with findings of gastric cancer. 
  There is evidence of extramural invasion."
  The Ground Truth column shows the reference CT slice with the radiologist-annotated lesion bounding box.
  The other columns display each model’s key-slice prediction.
  The white double circle symbol indicates predictions that match the ground-truth slice range.}
  \label{fig:lesion_aware_inst6}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst7_2.png}
  \caption{Visualization of lesion-aware slice selection for the \replaced{finding: }{finding 
  }"Multiple enlarged lymph nodes are observed in the celiac artery region, which is considered metastatic."
  \replaced{In}{For} this \replaced{case,}{example,} no prediction fell within the ground-truth range.}
  \label{fig:lesion_aware_inst7}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst8_2.png}
  \caption{Visualization of lesion-aware slice selection for the \replaced{finding: 
}{finding 
  }"Ring enhancement lesions are observed in both lobes of the liver, indicating the presence of hepatic metastases."}
  \label{fig:lesion_aware_inst8}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_lesion_aware_inst10_2.png}
  \caption{Visualization of lesion-aware slice selection for the \replaced{finding:}{finding} "Prostate enlargement is observed."}
  \label{fig:lesion_aware_inst10}
\end{figure}


\clearpage


\subsection{Organ Awareness}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_organ_aware_liver_2.png}
  \caption{Similarity profile between the prompt “Liver” and each axial CT slice. 
  The green line denotes the ground-truth slice range. 
  The dashed line indicates the Youden-index threshold determined on the validation set.}
  \label{fig:organ_aware_liver}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{./figures/app_organ_aware_kidney_2.png}
  \caption{Similarity profile between the prompt “Kidney” and each axial CT slice.}
  \label{fig:organ_aware_kidney}
\end{figure}

\clearpage

\section{Impact of Organ-Aware Synthetic Data}\label{app:impact}

  To \replaced{evaluate}{highlight} the contribution of synthetic data generated from organ labels, we conducted an ablation study in which the models were fine-tuned only on report-derived lesion sentences, without using any organ-aware synthetic data. 
  Because the training set in this condition contained only 625 sentence-slice pairs (Fig. \ref{fig:dataset_detail}), we adopted a milder fine-tuning schedule with \replaced{the}{a} learning rate decayed from 1e-5 to 1e-6 over 20 epochs. 
  \replaced{This}{We} \replaced{model was referred}{refer} to\deleted{ this model} as BiomedCLIP(lesion-FT). 
  Table \ref{tab:impact} compares the \replaced{lesion-awareness}{lesion-aware} \replaced{accuracies}{accuracy} of BiomedCLIP, BiomedCLIP(lesion-FT), and BiomedCLIP(FT).
  The results show that BiomedCLIP(lesion-FT) achieved a higher Top-1 accuracy than the original BiomedCLIP baseline (45.98\% vs. 44.83\replaced{\%),}{\%)} but remained inferior to BiomedCLIP(FT), which incorporated organ-aware synthetic data (51.72\%).
  These findings indicate that synthetic data are beneficial not only for organ-aware \replaced{evaluation,}{evaluation} but also for improving lesion-aware performance.


\begin{table}[ht]
  \centering
  \caption{\added{New table added in revision.} Lesion-aware accuracy [\%] of BiomedCLIP variants. BiomedCLIP(lesion-FT) was fine-tuned only on lesion sentences without synthetic data.}
  \label{tab:impact}
  \begin{tabular}{lcc}
    \toprule
    Method                  & Acc.@1,↑      & Acc.@5,↑       \\
    \midrule
    BiomedCLIP              & 44.83       & 60.92        \\
    BiomedCLIP(FT)          & 51.72       & 64.37        \\
    BiomedCLIP(lesion-FT)   & 45.98       & 63.22        \\
    \bottomrule
  \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\isPreprints{}{% This command is only used for ``preprints''.
\begin{adjustwidth}{-\extralength}{0cm}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\bibliography{references}


% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{
%Reviewer 1 comments and authors’ response
%Reviewer 2 comments and authors’ response
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
%\isPreprints{}{% This command is only used for ``preprints''.
\end{adjustwidth}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
\end{document}


